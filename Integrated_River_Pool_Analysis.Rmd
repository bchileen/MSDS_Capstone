---
title: "Integrated River and Pool-Based Shoaling Prediction"
subtitle: "USACE Rock Island District - Upper Mississippi River & Illinois Waterway"
author: "Barrie Chileen Martinez"
output: html_document
date: "2025"
---

## Overview

This script integrates **pool-based modeling** into your existing river-level framework.
The approach:

1. **Preserves your original river-level analysis** (IWW_data, Miss_data, Combined)
2. **Adds pool-level modeling** as an additional analytical layer
3. **Maintains LSTM** for both river and pool levels
4. **Adds XGBoost feature selection** to identify the best gages per pool
5. **Compares performance** between river-level and pool-level predictions

---

## Setup and Load Packages

```{r setup, message=FALSE, warning=FALSE}
# Core data manipulation and visualization
library(tidyverse)
library(lubridate)
library(zoo)
library(knitr)
library(kableExtra)

# Machine learning frameworks
library(caret)
library(xgboost)

# Time series and deep learning
library(keras3)
library(tensorflow)
library(forecast)

# Visualization
library(ggplot2)
library(patchwork)
library(viridis)
library(corrplot)

# Parallel processing
library(doParallel)
library(foreach)

cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

# Create output directories
dir.create("./outputs", showWarnings = FALSE)
dir.create("./outputs/river_level", showWarnings = FALSE)
dir.create("./outputs/pool_level", showWarnings = FALSE)
dir.create("./outputs/comparison", showWarnings = FALSE)

cat("Setup complete\n")
```

---

## Load and Clean Data 

```{r load_data}
# Load datasets (your original code)
gage_data <- read_csv("UMR_IWW_1999_2024.csv", show_col_types = FALSE)
CSAT_data <- read_csv("CSAT_DATA_Combined.csv", show_col_types = FALSE)
gage_metadata <- read_csv("gage_metadata.csv", show_col_types = FALSE)

# Filter gages by river (your original code)
IWW_gages <- gage_metadata |> filter(River == "IWW") |> pull(Gage_ID)
Miss_gages <- gage_metadata |> filter(River == "Miss") |> pull(Gage_ID)
All_gages <- gage_metadata |> pull(Gage_ID)

cat("Data loaded successfully\n")
```

```{r clean_gage_data}
# Clean gage data 
gage_data_cleaned <- gage_data |>
  mutate(
    Date = dmy(Date),
    Year = year(Date),
    Month = month(Date),
    Day = yday(Date),
    WeekOfYear = week(Date),
    Season = case_when(
      Month %in% c(12, 1, 2) ~ "Winter",
      Month %in% c(3, 4, 5) ~ "Spring",
      Month %in% c(6, 7, 8) ~ "Summer",
      Month %in% c(9, 10, 11) ~ "Fall"
    ),
    Season = factor(Season, levels = c("Winter", "Spring", "Summer", "Fall"))
  ) %>%
  filter(!is.na(Date)) |>
  arrange(Date)

# Interpolate missing values (your original code)
gage_cols <- setdiff(names(gage_data_cleaned), 
                     c("Date", "Year", "Month", "Day", "WeekOfYear", "Season"))

gage_data_interpolated <- gage_data_cleaned %>%
  mutate(across(all_of(gage_cols), ~ na.approx(.x, x = Date, maxgap = 4, na.rm = FALSE)))

cat("Missing values before:", sum(is.na(gage_data_cleaned[, gage_cols])), "\n")
cat("Missing values after:", sum(is.na(gage_data_interpolated[, gage_cols])), "\n")

gage_data_no_na <- gage_data_interpolated|>
  drop_na()
  
```

```{r clean_csat_data}
# Clean CSAT data 
CSAT_data_cleaned <- CSAT_data |>
  mutate(
    SurveyDateBefore = ymd(as.character(SurveyDateBefore)),
    SurveyDateAfter = ymd(as.character(SurveyDateAfter)),
    DaysBetween = as.numeric(SurveyDateAfter - SurveyDateBefore),
    SurveyYear = year(SurveyDateAfter),
    SurveyMonth = month(SurveyDateAfter),
    SurveyWeekOfYear = week(SurveyDateAfter),
    SurveySeason = case_when(
      SurveyMonth %in% c(12, 1, 2) ~ "Winter",
      SurveyMonth %in% c(3, 4, 5) ~ "Spring",
      SurveyMonth %in% c(6, 7, 8) ~ "Summer",
      SurveyMonth %in% c(9, 10, 11) ~ "Fall"
    ),
    SurveySeason = factor(SurveySeason, levels = c("Winter", "Spring", "Summer", "Fall")),
    rate_reliability = case_when(
      DaysBetween <= 30 ~ 1.0,
      DaysBetween <= 60 ~ 0.8,
      DaysBetween <= 180 ~ 0.6,
      DaysBetween <= 240 ~ 0.4,
      TRUE ~ 0.2
    )
  ) |>
  filter(
    !is.na(AnnualShoalingRate_ftperyr),
    !is.infinite(AnnualShoalingRate_ftperyr),
    abs(AnnualShoalingRate_ftperyr) <= 30,
    DaysBetween <= 365
  )

cat("CSAT data cleaned:", nrow(CSAT_data_cleaned), "observations\n")
```

```{r join_data_original}
# Join data 
gage_CSAT_joined <- CSAT_data_cleaned |>
  left_join(gage_data_interpolated, by = c("SurveyDateAfter" = "Date")) |>
  select(-SurveyYear, -SurveyMonth, -SurveyWeekOfYear)

# Filter out pools with insufficient data 
gage_CSAT_joined <- gage_CSAT_joined |>
  filter(!pool %in% c("AL", "LP", "BR", "CS", "24"))

cat("Joined dataset:", nrow(gage_CSAT_joined), "observations\n")
```

---

## RIVER-LEVEL ANALYSIS 
```{r river_level_splits}
# Create river-level datasets 
IWW_data <- gage_CSAT_joined %>% filter(river == "IL")
Miss_data <- gage_CSAT_joined %>% filter(river == "UM")

# Temporal splits function (your original code)
create_temporal_splits <- function(data, train_prop = 0.70, val_prop = 0.15) {
  data <- data |> arrange(SurveyDateAfter)
  n <- nrow(data)
  train_end <- floor(n * train_prop)
  val_end <- floor(n * (train_prop + val_prop))
  
  splits <- list(
    train_idx = 1:train_end,
    val_idx = (train_end + 1):val_end,
    test_idx = (val_end + 1):n,
    train_dates = data$SurveyDateAfter[1:train_end],
    val_dates = data$SurveyDateAfter[(train_end + 1):val_end],
    test_dates = data$SurveyDateAfter[(val_end + 1):n]
  )
  
  cat(sprintf("  Train: %d, Val: %d, Test: %d\n",
              length(splits$train_idx), length(splits$val_idx), length(splits$test_idx)))
  return(splits)
}

cat("\n=== RIVER-LEVEL TEMPORAL SPLITS ===\n")
cat("Illinois Waterway:\n")
IWW_splits <- create_temporal_splits(IWW_data)
cat("Mississippi River:\n")
Miss_splits <- create_temporal_splits(Miss_data)
cat("Combined:\n")
Combined_splits <- create_temporal_splits(gage_CSAT_joined)
```

### River-Level xGBoost (Your Original Code)

```{r river_xgboost}
train_xgboost_temporal <- function(data, splits, gages_to_use, dataset_name) {
  
  cat(sprintf("\n=== Training xGBoost: %s ===\n", dataset_name))
  
  # Prepare features
  xgb_data <- data |>
    select(AnnualShoalingRate_ftperyr, WeekOfYear, any_of(gages_to_use)) |>
    drop_na()
  
  # Temporal CV setup
  n_samples <- nrow(xgb_data)
  n_folds <- 5
  
  temporal_indices <- lapply(1:n_folds, function(i) {
    test_start <- floor(n_samples * i / (n_folds + 1))
    test_end <- floor(n_samples * (i + 1) / (n_folds + 1))
    list(train = 1:(test_start - 1), test = test_start:test_end)
  })
  temporal_indices <- temporal_indices[sapply(temporal_indices, function(x) length(x$train) > 50)]
  
  ctrl_temporal <- trainControl(
    method = "cv",
    number = length(temporal_indices),
    index = lapply(temporal_indices, `[[`, "train"),
    indexOut = lapply(temporal_indices, `[[`, "test"),
    verboseIter = FALSE,
    allowParallel = TRUE
  )
  
  tune_grid <- expand.grid(
    nrounds = c(100, 200),
    max_depth = c(4, 6, 8),
    eta = c(0.05, 0.1, 0.2),
    gamma = c(0, 0.1),
    colsample_bytree = c(0.6, 0.8),
    min_child_weight = c(1, 3),
    subsample = c(0.8, 1.0)
  )
  
  set.seed(118)
  xgb_model <- caret::train(
    AnnualShoalingRate_ftperyr ~ .,
    data = xgb_data,
    method = "xgbTree",
    tuneGrid = tune_grid,
    trControl = ctrl_temporal,
    metric = "RMSE",
    verbose = FALSE
  )
  
  # Test predictions
  test_data <- data[splits$test_idx, ] |>
    select(AnnualShoalingRate_ftperyr, WeekOfYear, any_of(gages_to_use), SurveyDateAfter) |>
    drop_na()
  
  test_pred <- predict(xgb_model, newdata = test_data)
  test_rmse <- sqrt(mean((test_pred - test_data$AnnualShoalingRate_ftperyr)^2))
  test_mae <- mean(abs(test_pred - test_data$AnnualShoalingRate_ftperyr))
  
  cat(sprintf("Test RMSE: %.4f, MAE: %.4f\n", test_rmse, test_mae))
  
  return(list(
    model = xgb_model,
    metrics = data.frame(
      Dataset = dataset_name, Model = "xGBoost", Level = "River",
      Test_RMSE = test_rmse, Test_MAE = test_mae
    ),
    predictions = data.frame(
      actual = test_data$AnnualShoalingRate_ftperyr,
      predicted = test_pred,
      date = test_data$SurveyDateAfter
    )
  ))
}

# Train river-level xGBoost models
cat("\n=== RIVER-LEVEL XGBOOST ===\n")
IWW_xgb_river <- train_xgboost_temporal(IWW_data, IWW_splits, IWW_gages, "IWW_River")
Miss_xgb_river <- train_xgboost_temporal(Miss_data, Miss_splits, Miss_gages, "Miss_River")
Combined_xgb_river <- train_xgboost_temporal(gage_CSAT_joined, Combined_splits, All_gages, "Combined_River")
```

### River-Level LSTM 

```{r river_lstm_prep}
# LSTM data preparation (your original code)
prepare_lstm_data_multistep <- function(data, splits, gages_to_use,
                                        sequence_length = 30,
                                        forecast_horizon = 45,
                                        dataset_name) {
  
  cat(sprintf("\n=== Preparing LSTM Data: %s ===\n", dataset_name))
  
  feature_data <- data |>
    select(SurveyDateAfter, AnnualShoalingRate_ftperyr, DaysBetween, any_of(gages_to_use)) |>
    arrange(SurveyDateAfter) |>
    drop_na()
  
  cat(sprintf("Data: %d observations, %d features\n", nrow(feature_data), ncol(feature_data) - 2))
  
  if(nrow(feature_data) < sequence_length + forecast_horizon + 10) {
    cat("WARNING: Insufficient data\n")
    return(NULL)
  }
  
  # Scale features
  gage_features <- feature_data |>
    select(-SurveyDateAfter, -AnnualShoalingRate_ftperyr) |>
    as.matrix()
  
  feature_means <- apply(gage_features, 2, mean, na.rm = TRUE)
  feature_sds <- apply(gage_features, 2, sd, na.rm = TRUE)
  feature_sds[feature_sds == 0] <- 1
  
  scaled_features <- scale(gage_features, center = feature_means, scale = feature_sds)
  target_values <- feature_data$AnnualShoalingRate_ftperyr
  dates <- feature_data$SurveyDateAfter
  
  # Create sequences
  n_samples <- nrow(scaled_features) - sequence_length - forecast_horizon + 1
  n_features <- ncol(scaled_features)
  
  X <- array(0, dim = c(n_samples, sequence_length, n_features))
  y <- matrix(0, nrow = n_samples, ncol = forecast_horizon)
  sample_dates <- character(n_samples)
  
  for(i in 1:n_samples) {
    X[i, , ] <- scaled_features[i:(i + sequence_length - 1), ]
    y[i, ] <- target_values[(i + sequence_length):(i + sequence_length + forecast_horizon - 1)]
    sample_dates[i] <- as.character(dates[i + sequence_length + forecast_horizon - 1])
  }
  
  # Split
  train_size <- floor(n_samples * 0.70)
  val_size <- floor(n_samples * 0.15)
  
  cat(sprintf("Sequences: %d total, Train: %d, Val: %d, Test: %d\n",
              n_samples, train_size, val_size, n_samples - train_size - val_size))
  
  return(list(
    X_train = X[1:train_size, , , drop = FALSE],
    X_val = X[(train_size + 1):(train_size + val_size), , , drop = FALSE],
    X_test = X[(train_size + val_size + 1):n_samples, , , drop = FALSE],
    y_train = y[1:train_size, ],
    y_val = y[(train_size + 1):(train_size + val_size), ],
    y_test = y[(train_size + val_size + 1):n_samples, ],
    dates_test = sample_dates[(train_size + val_size + 1):n_samples],
    scalers = list(means = feature_means, sds = feature_sds),
    sequence_length = sequence_length,
    forecast_horizon = forecast_horizon,
    n_features = n_features,
    dataset_name = dataset_name
  ))
}

# Build LSTM model (your original code)
build_lstm <- function(sequence_length, n_features, forecast_horizon) {
  model <- keras_model_sequential() |>
    layer_lstm(units = 128, return_sequences = TRUE,
               input_shape = c(sequence_length, n_features),
               dropout = 0.2, recurrent_dropout = 0.2) |>
    layer_lstm(units = 64, return_sequences = FALSE,
               dropout = 0.2, recurrent_dropout = 0.2) |>
    layer_dense(units = 64, activation = 'relu') |>
    layer_dropout(rate = 0.3) |>
    layer_dense(units = forecast_horizon, activation = 'linear')
  
  model |> compile(
    optimizer = optimizer_adam(learning_rate = 0.001),
    loss = 'mse',
    metrics = c('mae')
  )
  return(model)
}

# Train LSTM (your original code)
train_lstm_multistep <- function(lstm_data, dataset_name) {
  if(is.null(lstm_data)) return(NULL)
  
  cat(sprintf("\n=== Training LSTM: %s ===\n", dataset_name))
  
  model <- build_lstm(lstm_data$sequence_length, lstm_data$n_features, lstm_data$forecast_horizon)
  
  early_stopping <- callback_early_stopping(monitor = 'val_loss', patience = 20, restore_best_weights = TRUE)
  reduce_lr <- callback_reduce_lr_on_plateau(monitor = 'val_loss', factor = 0.5, patience = 10, min_lr = 1e-6)
  
  history <- model |> fit(
    x = lstm_data$X_train, y = lstm_data$y_train,
    validation_data = list(lstm_data$X_val, lstm_data$y_val),
    epochs = 100, batch_size = 32,
    callbacks = list(early_stopping, reduce_lr),
    verbose = 1
  )
  
  test_pred <- predict(model, lstm_data$X_test)
  test_rmse <- sqrt(mean((test_pred - lstm_data$y_test)^2))
  test_mae <- mean(abs(test_pred - lstm_data$y_test))
  
  cat(sprintf("Test RMSE: %.4f, MAE: %.4f\n", test_rmse, test_mae))
  
  return(list(
    model = model,
    history = history,
    metrics = data.frame(
      Dataset = dataset_name, Model = "LSTM", Level = "River",
      Test_RMSE = test_rmse, Test_MAE = test_mae
    ),
    predictions = list(actual = lstm_data$y_test, predicted = test_pred, dates = lstm_data$dates_test)
  ))
}
```

```{r train_river_lstm}
# Prepare and train river-level LSTM
cat("\n=== RIVER-LEVEL LSTM ===\n")

IWW_lstm_data_river <- prepare_lstm_data_multistep(
  IWW_data, IWW_splits, IWW_gages,
  sequence_length = 20, forecast_horizon = 45, dataset_name = "IWW_River"
)

Miss_lstm_data_river <- prepare_lstm_data_multistep(
  Miss_data, Miss_splits, Miss_gages,
  sequence_length = 30, forecast_horizon = 45, dataset_name = "Miss_River"
)

Combined_lstm_data_river <- prepare_lstm_data_multistep(
  gage_CSAT_joined, Combined_splits, All_gages,
  sequence_length = 25, forecast_horizon = 45, dataset_name = "Combined_River"
)

# Train LSTM models
IWW_lstm_river <- train_lstm_multistep(IWW_lstm_data_river, "IWW_River")
Miss_lstm_river <- train_lstm_multistep(Miss_lstm_data_river, "Miss_River")
Combined_lstm_river <- train_lstm_multistep(Combined_lstm_data_river, "Combined_River")
```

---

## PART B: POOL-LEVEL ANALYSIS (New Addition)

This section adds pool-level modeling while preserving your river-level work.

### B1: Create Pool-Level Aggregated Data

```{r pool_aggregation}
cat("\n")
cat("╔══════════════════════════════════════════════════════════════════╗\n")
cat("║                  POOL-LEVEL ANALYSIS                             ║\n")
cat("╚══════════════════════════════════════════════════════════════════╝\n")

# Aggregate reach-level data to pool level
pool_level_data <- CSAT_data_cleaned |>
  group_by(pool, river, SurveyDateAfter) |>
  summarise(
    Pool_ShoalingRate_ftyr = weighted.mean(
      AnnualShoalingRate_ftperyr,
      w = SurveyOverlapArea_sqft,
      na.rm = TRUE
    ),
    Pool_VolumeChange_CY = sum(NetVolumeChange_CY, na.rm = TRUE),
    N_Reaches = n(),
    Avg_DaysBetween = mean(DaysBetween, na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(
    Dredge_Need = as.factor(ifelse(Pool_ShoalingRate_ftyr > 0, "Yes", "No"))
  ) |>
  filter(!pool %in% c("AL", "LP", "BR", "CS", "24"))

# Join with gage data
pool_model_data <- pool_level_data |>
  left_join(gage_data_interpolated, by = c("SurveyDateAfter" = "Date"))

# Pool summary
pool_summary <- pool_model_data |>
  group_by(pool, river) |>
  summarise(
    N_Obs = n(),
    Mean_Rate = round(mean(Pool_ShoalingRate_ftyr, na.rm = TRUE), 3),
    Pct_Dredge = round(mean(Dredge_Need == "Yes") * 100, 1),
    .groups = "drop"
  )

cat("\n=== POOL-LEVEL DATA SUMMARY ===\n")
print(pool_summary)

# Identify pools with sufficient data
sufficient_pools <- pool_summary |>
  filter(N_Obs >= 40) |> 
  pull(pool)
cat("\nPools with >=40 observations:", paste(sufficient_pools, collapse = ", "), "\n")
```

### B2: Pool-Specific Gage Selection via XGBoost

```{r pool_feature_selection}
# Function to get candidate gages for a pool
get_pool_candidate_gages <- function(pool_name, metadata, n_upstream = 20) {
  pool_info <- metadata |> filter(Pool == pool_name)
  if(nrow(pool_info) == 0) return(character(0))
  
  river_type <- unique(pool_info$River)[1]
  pool_gages <- pool_info$Gage_ID
  
  # Add upstream gages
  target_mile <- max(pool_info$RiverMile, na.rm = TRUE)
  upstream <- metadata |>
    filter(River == river_type, RiverMile > target_mile) |>
    arrange(RiverMile) |>
    head(n_upstream)|>
    pull(Gage_ID)
  
  return(unique(c(pool_gages, upstream)))
}

# XGBoost feature selection function
select_pool_gages_xgb <- function(data, pool_name, candidate_gages, n_top = 8) {
  
  available_gages <- intersect(candidate_gages, names(data))
  if(length(available_gages) == 0) return(NULL)
  
  pool_data <- data |>
    filter(pool == pool_name) |>
    select(Pool_ShoalingRate_ftyr, WeekOfYear, all_of(available_gages))|>
    drop_na()
  
  if(nrow(pool_data) < 50) return(NULL)
  
  X <- as.matrix(pool_data |> 
                   select(-Pool_ShoalingRate_ftyr))
  y <- pool_data$Pool_ShoalingRate_ftyr
  
  dtrain <- xgb.DMatrix(data = X, label = y)
  
  set.seed(118)
  xgb_model <- xgb.train(
    params = list(objective = "reg:squarederror", max_depth = 6, eta = 0.1),
    data = dtrain, nrounds = 100, verbose = 0
  )
  
  importance <- xgb.importance(model = xgb_model)
  top_gages <- setdiff(importance$Feature[1:min(n_top, nrow(importance))], "WeekOfYear")
  
  return(list(pool = pool_name, importance = importance, top_gages = top_gages))
}

# Run feature selection for each pool
cat("\n=== POOL-LEVEL FEATURE SELECTION ===\n")
pool_feature_selection <- list()

for(p in sufficient_pools) {
  candidates <- get_pool_candidate_gages(p, gage_metadata, n_upstream = 5)
  candidates <- intersect(candidates, names(pool_model_data))
  
  if(length(candidates) > 0) {
    result <- select_pool_gages_xgb(pool_model_data, p, candidates, n_top = 6)
    if(!is.null(result)) {
      pool_feature_selection[[p]] <- result
      cat(sprintf("Pool %s: Selected %d gages - %s\n", 
                  p, length(result$top_gages), paste(result$top_gages, collapse = ", ")))
    }
  }
}
```

### B3: Pool-Level xGBoost with Selected Features

```{r pool_xgboost}
train_pool_xgboost <- function(data, pool_name, selected_gages) {
  
  pool_data <- data |>
    filter(pool == pool_name) |>
    arrange(SurveyDateAfter)
  
  available_gages <- intersect(selected_gages, names(pool_data))
  if(length(available_gages) == 0) return(NULL)
  
  model_data <- pool_data |>
    select(Pool_ShoalingRate_ftyr, Dredge_Need, WeekOfYear, all_of(available_gages)) |>
    drop_na()
  
  if(nrow(model_data) < 40) return(NULL)
  
  # Temporal split
  n <- nrow(model_data)
  train_idx <- 1:floor(n * 0.80)
  test_idx <- (floor(n * 0.80) + 1):n
  
  # Regression
  X_train <- as.matrix(model_data[train_idx, ] |>
                         select(-Pool_ShoalingRate_ftyr, -Dredge_Need))
  y_train <- model_data$Pool_ShoalingRate_ftyr[train_idx]
  X_test <- as.matrix(model_data[test_idx, ] |>
                        select(-Pool_ShoalingRate_ftyr, -Dredge_Need))
  y_test <- model_data$Pool_ShoalingRate_ftyr[test_idx]
  
  dtrain <- xgb.DMatrix(data = X_train, label = y_train)
  dtest <- xgb.DMatrix(data = X_test, label = y_test)
  
  set.seed(118)
  xgb_reg <- xgb.train(
    params = list(objective = "reg:squarederror", max_depth = 5, eta = 0.1),
    data = dtrain, nrounds = 150,
    watchlist = list(test = dtest),
    early_stopping_rounds = 20, verbose = 0
  )
  
  reg_pred <- predict(xgb_reg, dtest)
  reg_rmse <- sqrt(mean((reg_pred - y_test)^2))
  reg_mae <- mean(abs(reg_pred - y_test))
  
  # Classification
  train_class <- model_data[train_idx, ] |>
    select(-Pool_ShoalingRate_ftyr)
  test_class <- model_data[test_idx, ] |>
    select(-Pool_ShoalingRate_ftyr)
  
  ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
  
  set.seed(118)
  xgb_class <- train(
    Dredge_Need ~ ., data = train_class,
    method = "xgbTree", trControl = ctrl, tuneLength = 3, verbose = FALSE
  )
  
  class_pred <- predict(xgb_class, test_class)
  class_prob <- predict(xgb_class, test_class, type = "prob")
  cm <- confusionMatrix(class_pred, test_class$Dredge_Need, positive = "Yes")
  
  return(list(
    pool = pool_name,
    reg_model = xgb_reg,
    class_model = xgb_class,
    reg_metrics = data.frame(
      Pool = pool_name, Model = "xGBoost", Level = "Pool",
      Test_RMSE = reg_rmse, Test_MAE = reg_mae
    ),
    class_metrics = data.frame(
      Pool = pool_name,
      Accuracy = cm$overall["Accuracy"],
      Recall = cm$byClass["Sensitivity"],
      Precision = cm$byClass["Pos Pred Value"]
    ),
    selected_gages = available_gages
  ))
}

# Train pool-level xGBoost
cat("\n=== POOL-LEVEL XGBOOST ===\n")
pool_xgb_models <- list()

for(p in names(pool_feature_selection)) {
  selected <- pool_feature_selection[[p]]$top_gages
  result <- tryCatch(
    train_pool_xgboost(pool_model_data, p, selected),
    error = function(e) NULL
  )
  if(!is.null(result)) {
    pool_xgb_models[[p]] <- result
    cat(sprintf("Pool %s: RMSE=%.4f, Recall=%.3f\n",
                p, result$reg_metrics$Test_RMSE, result$class_metrics$Recall))
  }
}
```

### B4: Pool-Level LSTM with Selected Features

```{r pool_lstm}
# Prepare pool-level LSTM data
prepare_pool_lstm_data <- function(data, pool_name, selected_gages,
                                    sequence_length = 20, forecast_horizon = 45) {
  
  available_gages <- intersect(selected_gages, names(data))
  if(length(available_gages) == 0) return(NULL)
  
  pool_data <- data |>
    filter(pool == pool_name) |>
    arrange(SurveyDateAfter) |>
    select(SurveyDateAfter, Pool_ShoalingRate_ftyr, WeekOfYear, all_of(available_gages)) |>
    drop_na()
  
  if(nrow(pool_data) < sequence_length + forecast_horizon + 20) return(NULL)
  
  # Scale features
  features <- pool_data |>
    select(-SurveyDateAfter, -Pool_ShoalingRate_ftyr) |> as.matrix()
  feature_means <- apply(features, 2, mean, na.rm = TRUE)
  feature_sds <- apply(features, 2, sd, na.rm = TRUE)
  feature_sds[feature_sds == 0] <- 1
  scaled_features <- scale(features, center = feature_means, scale = feature_sds)
  
  target <- pool_data$Pool_ShoalingRate_ftyr
  dates <- pool_data$SurveyDateAfter
  
  # Create sequences
  n_samples <- nrow(scaled_features) - sequence_length - forecast_horizon + 1
  n_features <- ncol(scaled_features)
  
  X <- array(0, dim = c(n_samples, sequence_length, n_features))
  y <- matrix(0, nrow = n_samples, ncol = forecast_horizon)
  
  for(i in 1:n_samples) {
    X[i, , ] <- scaled_features[i:(i + sequence_length - 1), ]
    y[i, ] <- target[(i + sequence_length):(i + sequence_length + forecast_horizon - 1)]
  }
  
  # Temporal split
  train_size <- floor(n_samples * 0.70)
  val_size <- floor(n_samples * 0.15)
  
  return(list(
    X_train = X[1:train_size, , , drop = FALSE],
    X_val = X[(train_size + 1):(train_size + val_size), , , drop = FALSE],
    X_test = X[(train_size + val_size + 1):n_samples, , , drop = FALSE],
    y_train = y[1:train_size, ],
    y_val = y[(train_size + 1):(train_size + val_size), ],
    y_test = y[(train_size + val_size + 1):n_samples, ],
    sequence_length = sequence_length,
    forecast_horizon = forecast_horizon,
    n_features = n_features,
    pool = pool_name
  ))
}

# Build pool-level LSTM (smaller architecture for less data)
build_pool_lstm <- function(sequence_length, n_features, forecast_horizon) {
  model <- keras_model_sequential() |>
    layer_lstm(units = 64, return_sequences = TRUE,
               input_shape = c(sequence_length, n_features),
               dropout = 0.2, recurrent_dropout = 0.2) |>
    layer_lstm(units = 32, return_sequences = FALSE,
               dropout = 0.2, recurrent_dropout = 0.2) |>
    layer_dense(units = 32, activation = 'relu') |>
    layer_dropout(rate = 0.3) |>
    layer_dense(units = forecast_horizon, activation = 'linear')
  
  model |> compile(
    optimizer = optimizer_adam(learning_rate = 0.001),
    loss = 'mse', metrics = c('mae')
  )
  return(model)
}

# Train pool-level LSTM
train_pool_lstm <- function(lstm_data) {
  if(is.null(lstm_data)) return(NULL)
  
  model <- build_pool_lstm(lstm_data$sequence_length, lstm_data$n_features, lstm_data$forecast_horizon)
  
  early_stop <- callback_early_stopping(monitor = 'val_loss', patience = 15, restore_best_weights = TRUE)
  
  history <- model |> fit(
    x = lstm_data$X_train, y = lstm_data$y_train,
    validation_data = list(lstm_data$X_val, lstm_data$y_val),
    epochs = 100, batch_size = 16,
    callbacks = list(early_stop), verbose = 0
  )
  
  test_pred <- predict(model, lstm_data$X_test)
  test_rmse <- sqrt(mean((test_pred - lstm_data$y_test)^2))
  test_mae <- mean(abs(test_pred - lstm_data$y_test))
  
  return(list(
    pool = lstm_data$pool,
    model = model,
    metrics = data.frame(
      Pool = lstm_data$pool, Model = "LSTM", Level = "Pool",
      Test_RMSE = test_rmse, Test_MAE = test_mae
    )
  ))
}

# Train LSTM for pools with enough data
cat("\n=== POOL-LEVEL LSTM ===\n")
pool_lstm_models <- list()

for(p in names(pool_feature_selection)) {
  selected <- pool_feature_selection[[p]]$top_gages
  
  lstm_data <- prepare_pool_lstm_data(
    pool_model_data, p, selected,
    sequence_length = 15, forecast_horizon = 45
  )
  
  if(!is.null(lstm_data) && dim(lstm_data$X_train)[1] >= 20) {
    result <- tryCatch(train_pool_lstm(lstm_data), error = function(e) NULL)
    if(!is.null(result)) {
      pool_lstm_models[[p]] <- result
      cat(sprintf("Pool %s: LSTM RMSE=%.4f\n", p, result$metrics$Test_RMSE))
    }
  }
}
```

---

## PART C: COMPARISON - River vs Pool Level

```{r comparison}
cat("\n")
cat("╔══════════════════════════════════════════════════════════════════╗\n")
cat("║              RIVER VS POOL LEVEL COMPARISON                      ║\n")
cat("╚══════════════════════════════════════════════════════════════════╝\n")

# Compile river-level results
river_results <- bind_rows(
  IWW_xgb_river$metrics,
  Miss_xgb_river$metrics,
  Combined_xgb_river$metrics,
  if(!is.null(IWW_lstm_river)) IWW_lstm_river$metrics else NULL,
  if(!is.null(Miss_lstm_river)) Miss_lstm_river$metrics else NULL,
  if(!is.null(Combined_lstm_river)) Combined_lstm_river$metrics else NULL
)

cat("\n=== RIVER-LEVEL RESULTS ===\n")
print(river_results)

# Compile pool-level results
pool_xgb_results <- bind_rows(lapply(pool_xgb_models, function(x) x$reg_metrics))
pool_lstm_results <- bind_rows(lapply(pool_lstm_models, function(x) x$metrics))
pool_results <- bind_rows(pool_xgb_results, pool_lstm_results)

cat("\n=== POOL-LEVEL RESULTS ===\n")
print(pool_results |> arrange(Model, Test_RMSE))

# Pool classification results
pool_class_results <- bind_rows(lapply(pool_xgb_models, function(x) x$class_metrics))
cat("\n=== POOL-LEVEL CLASSIFICATION ===\n")
print(pool_class_results )

# Summary comparison
cat("\n=== SUMMARY COMPARISON ===\n")
cat("\nRiver-Level (Average RMSE):\n")
river_summary <- river_results |>
  group_by(Model) |>
  summarise(Avg_RMSE = mean(Test_RMSE), Avg_MAE = mean(Test_MAE), .groups = "drop")
print(river_summary)

cat("\nPool-Level (Average RMSE):\n")
pool_summary_metrics <- pool_results |>
  group_by(Model) |>
  summarise(Avg_RMSE = mean(Test_RMSE), Avg_MAE = mean(Test_MAE), N_Pools = n(), .groups = "drop")
print(pool_summary_metrics)

# Save comparison
write_csv(river_results, "./outputs/comparison/River_Level_Results.csv")
write_csv(pool_results, "./outputs/comparison/Pool_Level_Results.csv")
write_csv(pool_class_results, "./outputs/comparison/Pool_Classification_Results.csv")
```

```{r comparison_plot, fig.width=12, fig.height=6}
# Visual comparison
all_results <- bind_rows(
  river_results |>
    mutate(Pool = Dataset),
  pool_results
)

comparison_plot <- ggplot(all_results, aes(x = reorder(Pool, Test_RMSE), y = Test_RMSE, fill = Level)) +
  geom_col(position = "dodge", width = 0.7) +
  facet_wrap(~Model, scales = "free_x") +
  coord_flip() +
  scale_fill_manual(values = c("River" = "steelblue", "Pool" = "darkorange")) +
  labs(
    title = "Model Performance: River-Level vs Pool-Level",
    subtitle = "Lower RMSE = Better Prediction",
    x = "Dataset / Pool",
    y = "Test RMSE (ft/yr)",
    fill = "Analysis Level"
  ) +
  theme_minimal()

print(comparison_plot)
ggsave("./outputs/comparison/River_vs_Pool_Comparison.png", comparison_plot, width = 12, height = 8, dpi = 300)
```

---

## PART D: Generate Predictions for Mapping

```{r generate_predictions}
# Generate current pool-level predictions
generate_pool_predictions <- function(pool_models, current_gage_data) {
  
  predictions <- data.frame()
  
  for(pool_name in names(pool_models)) {
    model <- pool_models[[pool_name]]
    gages <- model$selected_gages
    
    latest <- current_gage_data |>
      filter(!is.na(Date)) |>
      slice_tail(n = 1) |>
      mutate(WeekOfYear = week(Date))
    
    available <- intersect(gages, names(latest))
    if(length(available) < length(gages)) next
    
    pred_data <- latest |>
      select(WeekOfYear, all_of(gages))
    if(any(is.na(pred_data))) next
    
    # Classification probability
    prob <- predict(model$class_model, pred_data, type = "prob")$Yes
    
    # Regression prediction
    X <- as.matrix(pred_data)
    shoaling <- predict(model$reg_model, X)
    
    predictions <- bind_rows(predictions, data.frame(
      Pool = pool_name,
      Dredge_Probability_Pct = round(prob * 100, 1),
      Predicted_ShoalingRate = round(shoaling, 3),
      Priority = case_when(prob >= 0.70 ~ "HIGH", prob >= 0.40 ~ "MEDIUM", TRUE ~ "LOW"),
      stringsAsFactors = FALSE
    ))
  }
  
  return(predictions |>
           arrange(desc(Dredge_Probability_Pct)))
}

current_predictions <- generate_pool_predictions(pool_xgb_models, gage_data_interpolated)

cat("\n=== CURRENT POOL PREDICTIONS ===\n")
print(current_predictions)

# Add recommendations
current_predictions <- current_predictions |>
  mutate(Recommendation = case_when(
    Priority == "HIGH" ~ "Schedule survey & prepare dredge",
    Priority == "MEDIUM" ~ "Monitor conditions",
    Priority == "LOW" ~ "Routine monitoring"
  ))

write_csv(current_predictions, "./outputs/pool_level/Current_Pool_Predictions.csv")
```

```{r priority_chart, fig.width=10, fig.height=8}
# Priority chart for stakeholders
if(nrow(current_predictions) > 0) {
  priority_chart <- current_predictions |>
    mutate(Priority = factor(Priority, levels = c("LOW", "MEDIUM", "HIGH"))) %>%
    ggplot(aes(x = reorder(Pool, Dredge_Probability_Pct), y = Dredge_Probability_Pct, fill = Priority)) +
    geom_col(width = 0.7) +
    geom_text(aes(label = paste0(Dredge_Probability_Pct, "%")), hjust = -0.1, size = 3.5) +
    coord_flip() +
    scale_fill_manual(values = c("LOW" = "#4CAF50", "MEDIUM" = "#FFC107", "HIGH" = "#F44336")) +
    scale_y_continuous(limits = c(0, 110)) +
    labs(
      title = "Predicted Dredging Need by Pool",
      subtitle = "USACE Rock Island District",
      x = "Pool", y = "Dredge Probability (%)",
      caption = paste("Generated:", Sys.Date())
    ) +
    theme_minimal()
  
  print(priority_chart)
  ggsave("./outputs/pool_level/Pool_Priority_Chart.png", priority_chart, width = 10, height = 8, dpi = 300)
}
```

---

## Final Summary

```{r final_summary}
cat("\n")
cat("╔══════════════════════════════════════════════════════════════════════════╗\n")
cat("║                        ANALYSIS COMPLETE                                 ║\n")
cat("╠══════════════════════════════════════════════════════════════════════════╣\n")
cat("║                                                                          ║\n")
cat("║  RIVER-LEVEL ANALYSIS (Original Approach):                               ║\n")
cat(sprintf("║    - IWW, Mississippi, Combined models trained                          ║\n"))
cat(sprintf("║    - Best LSTM RMSE: %.4f ft/yr                                         ║\n",
            min(river_results$Test_RMSE[river_results$Model == "LSTM"], na.rm = TRUE)))
cat("║                                                                          ║\n")
cat("║  POOL-LEVEL ANALYSIS (New Addition):                                     ║\n")
cat(sprintf("║    - %d pools modeled with XGBoost                                       ║\n", length(pool_xgb_models)))
cat(sprintf("║    - %d pools modeled with LSTM                                          ║\n", length(pool_lstm_models)))
cat(sprintf("║    - Best pool RMSE: %.4f ft/yr                                          ║\n",
            min(pool_results$Test_RMSE, na.rm = TRUE)))
cat("║                                                                          ║\n")
cat("║  OUTPUT FILES:                                                           ║\n")
cat("║    ./outputs/river_level/     - River-level model results                ║\n")
cat("║    ./outputs/pool_level/      - Pool-level predictions & charts          ║\n")
cat("║    ./outputs/comparison/      - River vs Pool comparison                 ║\n")
cat("║                                                                          ║\n")
cat("║  KEY INSIGHT:                                                            ║\n")
cat("║    Pool-level models provide actionable predictions for dredge routing   ║\n")
cat("║    River-level LSTM captures broader temporal patterns                   ║\n")
cat("║    BOTH approaches complement each other for operational decisions       ║\n")
cat("║                                                                          ║\n")
cat("╚══════════════════════════════════════════════════════════════════════════╝\n")

# Cleanup
stopCluster(cl)
```
