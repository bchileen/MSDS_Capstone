---
title: "Predictive Dredging Models for Upper Mississippi River and Illinois Waterway"
subtitle: "Capstone Project - Machine Learning for Shoaling Rate Forecasting"
author: "Barrie Chileen Martinez"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    code_folding: show
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 12,
  fig.height = 8
)
```

# Loading in Code and Packages

```{r load-packages}
# Core data manipulation and visualization
library(tidyverse)
library(lubridate)
library(knitr)
library(kableExtra)

# Machine learning frameworks
library(caret)
library(xgboost)

# Time series and deep learning
library(keras)
library(keras3)
library(tensorflow)
library(zoo)
library(forecast)

# Statistical analysis and visualization
library(corrplot)
library(ggfortify)
library(viridis)
library(patchwork)
library(gridExtra)
library(scales)

# Parallel processing
library(doParallel)
library(foreach)

```

# Create Output directories and configure parallel processing

```{r}
# Set random seed for reproducibility
set.seed(118)

# Set up parallel processing
n_cores <- max(1, detectCores() - 1)
cl <- makeCluster(n_cores)
registerDoParallel(cl)
n_cores

# Create output directories
output_dirs <- c(
"./Output",
"./Output/EDA",
"./Output/PCA",
"./Output/xGBoost",
"./Output/LSTM",
"./Output/Pool_Models",
"./Output/Comparisons",
"./Output/Maps"
)

for (dir in output_dirs) {
dir.create(dir, showWarnings = FALSE, recursive = TRUE)
}

```

# Load Raw Data

```{r}
# Load datasets
gage_data <- read_csv("UMR_IWW_1999_2024.csv", show_col_types = FALSE)
CSAT_data <- read_csv("CSAT_DATA_Combined.csv", show_col_types = FALSE)
gage_metadata <- read_csv("gage_metadata.csv", show_col_types = FALSE)
dredge_data <- read_csv("Dredge_Event_data.csv", show_col_types=FALSE)

# Data Overview

cat("Gage Data:\n")
cat("Dimensions:", nrow(gage_data), "rows x", ncol(gage_data), "columns\n")
cat("Date range:", as.character(min(dmy(gage_data$Date), na.rm = TRUE)), "to",
    as.character(max(dmy(gage_data$Date), na.rm = TRUE)),"\n\n")

cat("CSAT Data:\n")
cat("Dimensions:", nrow(CSAT_data), "rows x", ncol(CSAT_data), "columns\n")
cat("Shoaling rate range:", 
    round(min(CSAT_data$AnnualShoalingRate_ftperyr, na.rm = TRUE), 3), "to",
    round(max(CSAT_data$AnnualShoalingRate_ftperyr, na.rm = TRUE), 3), "ft/yr\n\n")

cat("Gage Metadata:\n")
cat("Total gages:", nrow(gage_metadata), "\n")
```

## Split Gages by River

```{r}
# Create gage lists for filtering
IWW_gages <- gage_metadata |>
filter(River == "IL") |>
pull(Gage_ID)

Miss_gages <- gage_metadata |>
filter(River == "UM") |>
pull(Gage_ID)

All_gages <- gage_metadata |>
pull(Gage_ID)

cat("Gage counts by river:\n")
cat("  Illinois Waterway:", length(IWW_gages), "gages\n")
cat("  Mississippi River:", length(Miss_gages), "gages\n")
cat("  Total:", length(All_gages), "gages\n")
```

## Clean Gage Data

```{r}
### All dates need to be made into a date object and lets create a variable for
### season of the year and factor it. The final step is to remove any rows with 
### an empty date since we wont be able to join them to the CSAT data
 gage_data_cleaned <- gage_data |>
    mutate(
      Date = dmy(Date),
      Year = year(Date),
      Month = month(Date),
      Day = yday(Date),
      WeekOfYear = week(Date),
      Season = case_when(
        Month %in% c(12, 1, 2) ~ "Winter",
        Month %in% c(3, 4, 5) ~ "Spring", 
        Month %in% c(6, 7, 8) ~ "Summer",
        Month %in% c(9, 10, 11) ~ "Fall"
      ),
      Season = factor(Season, levels = c("Winter", "Spring", "Summer", "Fall"))
    ) |>
    filter(!is.na(Date)) |>
    arrange(Date)

### Now lets take a look at NA counts 
na_counts <- colSums(is.na(gage_data_cleaned))
names<-names(gage_data_cleaned)
cat("Missing values before interpolation:", 
    sum(is.na(select(gage_data_cleaned, -Date, -Year, -Month, -Day, -WeekOfYear, -Season))), "\n")


### Thats not too bad, lets try interpolating some dates. Lets use a gap of 4 
### days which is reasonable for river gage forecasts
gage_cols <- setdiff(names(gage_data_cleaned), c("Date", "Year", "Month", "Day", "WeekOfYear", "Season"))

# Apply interpolation to each gage column
gage_data_interpolated <- gage_data_cleaned |>
mutate(across(
  all_of(gage_cols),
  ~ na.approx(.x, x = Date, maxgap = 4, na.rm = FALSE)
))

### Print the missing values pre and post interpolation
cat("Missing values after interpolation:", 
    sum(is.na(select(gage_data_interpolated, all_of(gage_cols)))), "\n")
```

## Clean CSAT Data

```{r}
### All dates need to be made into a date object and lets create a variable for
### season of the year and factor it. Add additional fields for days between 
### surveys as well as a reliability indicator. Filter empty shoaling rates and
### make sure that the shoaling rate is <30 and the days between are within 1 year
 CSAT_data_cleaned <- CSAT_data |>
    mutate(
      SurveyDateBefore = ymd(as.character(SurveyDateBefore)),
      SurveyDateAfter = ymd(as.character(SurveyDateAfter)),
      DaysBetween = as.numeric(SurveyDateAfter - SurveyDateBefore),
      rate_reliability = case_when(DaysBetween <= 30 ~ 1.0,
  DaysBetween <= 60 ~ 0.8,  # High quality
  DaysBetween <= 180 ~ 0.6,  # Medium quality  
  DaysBetween <= 240 ~ 0.4,  # Low quality
  TRUE ~ 0.2  # Very low quality
)
    ) |>
    filter(!is.na(AnnualShoalingRate_ftperyr), 
           !is.infinite(AnnualShoalingRate_ftperyr),
           abs(AnnualShoalingRate_ftperyr)<= 30, 
           DaysBetween <= 365)

```

## Join the Datasets

```{r join-data}
# Join CSAT with gage data on survey date
gage_CSAT_joined <- CSAT_data_cleaned |>
left_join(
  gage_data_interpolated,
  by = c("SurveyDateBefore" = "Date")
) 

# Check pool sample counts
pool_counts <- gage_CSAT_joined |>
group_by(pool) |>
summarise(count = n(), .groups = "drop") |>
arrange(desc(count))

cat("Pool sample counts:\n")
print(pool_counts)

# Filter out pools with insufficient data
pools_to_exclude <- c("AL", "LP", "BR", "CS", "24")
gage_CSAT_joined <- gage_CSAT_joined |>
filter(!pool %in% pools_to_exclude)

cat("\nAfter filtering pools:", nrow(gage_CSAT_joined), "observations\n")
cat("Date range:", as.character(min(gage_CSAT_joined$SurveyDateBefore)), "to",
    as.character(max(gage_CSAT_joined$SurveyDateBefore)), "\n")
```

### Create River-Specific Datasets

```{r river-datasets}
# Split data by river system
IWW_data <- gage_CSAT_joined |>
filter(river == "IL") |>
mutate(pool = as.factor(pool))

Miss_data <- gage_CSAT_joined |>
filter(river == "UM") |>
mutate(pool = as.factor(pool))

cat("Dataset sizes:\n")
cat("  Illinois Waterway:", nrow(IWW_data), "observations\n")
cat("  Mississippi River:", nrow(Miss_data), "observations\n")
cat("  Combined:", nrow(gage_CSAT_joined), "observations\n")
```

# Exploratory Data Analysis

## Distribution Plots

```{r distrib plots, fig.height=10}
### Lets make a series of plots to view the distribution of the data
target_plots <- list()

# Custom labels for rivers
custom_labels <- c(
      "IL" = "Illinois Waterway",
      "UM" = "Mississippi River"
    )

# 1. Distribution
target_plots$dist <- gage_CSAT_joined |>
  ggplot(aes(x =  AnnualShoalingRate_ftperyr)) +
  geom_histogram(bins = 50, fill = "steelblue", alpha = 0.7) +
  geom_vline(aes(xintercept = median( AnnualShoalingRate_ftperyr, na.rm = TRUE)), 
             color = "red", linetype = "dashed", size = 1) +
  labs(title = "Distribution of Annual Shoaling Rate(ft/yr)",
       x = "Annual Shoaling Rate(ft/yr)",
       y = "Frequency") +
  theme_minimal()

# Full time series 
target_plots$ts <- gage_CSAT_joined |>
  ggplot(aes(x = SurveyDateBefore, y = AnnualShoalingRate_ftperyr)) +
  geom_line(alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "loess", span = 0.3, se = FALSE, color = "red", size = 1) +
  facet_wrap(~river, scales = "free_y", labeller = as_labeller(custom_labels)) +
  labs(title = "Shoaling Rate Time Series by River",
       x = "Date", y = "7-Day Average Shoaling Rate") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# 3. Seasonal patterns
target_plots$seasonal <- gage_CSAT_joined |>
  group_by(Month, river) |>
  summarise(avg_shoaling = mean(AnnualShoalingRate_ftperyr, na.rm = TRUE), .groups = "drop") |>
  ggplot(aes(x = Month, y = avg_shoaling, color = river)) +
    scale_color_manual(values = c("IL" = "orange", "UM" = "dodgerblue"),
    labels = c("IL" = "Illinois Waterway", "UM" = "Mississippi River"),
    name = "River")+
  geom_line(size = 1) +
  geom_point(size = 2) +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  labs(title = "Seasonal Shoaling Patterns",
       x = "Month", y = "Average Shoaling Rate",
       color = "River") +
  theme_minimal()

target_plots$seasonal

ggsave("./Output/EDA/SeasonalRates.png", width = 8, height = 3)

# Display plots
grid.arrange(target_plots$dist, target_plots$seasonal, ncol = 1)
target_plots$ts

ggplot(gage_CSAT_joined,aes(x = SurveyDateBefore, y = AnnualShoalingRate_ftperyr, color = river))+
         geom_line()+ 
  scale_color_manual(values = c("IL" = "orange", "UM" = "dodgerblue"),
    labels = c("IL" = "Illinois Waterway", "UM" = "Mississippi River"),
    name = "River")+ 
  labs(x = "Survey Date", y = "Annual Shoaling Rate (ft/yr)", title = "Annual Shoaling Rates By Pool")+
  facet_wrap(~pool)

ggsave("./Output/EDA/Pool_Timeseries.png", width = 10, height = 8)
```

## Dredge Data Exploration (Need to establish shoaling threshold)

```{r}
dredge_clean <- dredge_data |>
  filter(EXECYEAR >= 1999) |>
  mutate(
    dredge_date = as.Date(mdy_hm(DATE_START)),
    river_code = case_when(
      RIVER == "Illinois_Waterway" ~ "IL",
      RIVER == "Mississippi_River" ~ "UM"),
    pool_clean = POOL) |>
  filter(!is.na(dredge_date))

CSAT_clean <- CSAT_data_cleaned |>
  mutate(
    SurveyDateBefore = ymd(as.character(SurveyDateBefore)),
    SurveyDateAfter = ymd(as.character(SurveyDateAfter)),
    river_code = river,
    pool_clean = pool
  )

# Match - dredge event occurred within 30 days AFTER survey ended
CSAT_pre_dredge <- CSAT_clean |>
  inner_join(
    dredge_clean |> 
      select(dredge_date, river_code, pool_clean, volume_dredged = VOLUMEDREDGED),
    by = c("river_code", "pool_clean"),
    relationship = "many-to-many") |>
  mutate(days_to_dredge = as.numeric(dredge_date - SurveyDateAfter)) |>
  filter(days_to_dredge >= 0, days_to_dredge <= 30) |>  # Within 30 days
  group_by(across(c(-dredge_date, -volume_dredged, -days_to_dredge))) |>
  slice_min(days_to_dredge, n = 1) |>  # Keep closest match
  ungroup()

comb_thresh = round(mean(CSAT_pre_dredge$AnnualShoalingRate_ftperyr),2)

cat("Matched CSAT records:", nrow(CSAT_pre_dredge), "\n\n")

# Quick histogram
hist(CSAT_pre_dredge$AnnualShoalingRate_ftperyr, 
     breaks = 50, 
     main = "Shoaling Rates Before Dredge Events",
     xlab = "Annual Shoaling Rate (ft/yr)",
     col = "steelblue")
abline(v = mean(CSAT_pre_dredge$AnnualShoalingRate_ftperyr, na.rm = TRUE), 
       col = "red", lwd = 2, lty = 2)

river_thresholds <- CSAT_pre_dredge |>
  group_by(river) |>
  summarise(
    n_events = n(),
    mean_rate = round(mean(AnnualShoalingRate_ftperyr, na.rm = TRUE), 2),
    median_rate = round(median(AnnualShoalingRate_ftperyr, na.rm = TRUE), 2),
    pct_positive = round(mean(AnnualShoalingRate_ftperyr > 0, na.rm = TRUE) * 100, 1)
  )
print(river_thresholds)

pool_thresholds <- CSAT_pre_dredge |>
  group_by(pool) |>
  summarise(
    n_events = n(),
    mean_rate = round(mean(AnnualShoalingRate_ftperyr, na.rm = TRUE), 2),
    median_rate = round(median(AnnualShoalingRate_ftperyr, na.rm = TRUE), 2),
    pct_positive = round(mean(AnnualShoalingRate_ftperyr > 0, na.rm = TRUE) * 100, 1)
  )
print(pool_thresholds)

reach_thresholds <-CSAT_pre_dredge |>
  group_by(pool, reach) |>
  summarise(
    n_events = n(),
    mean_rate = round(mean(AnnualShoalingRate_ftperyr, na.rm = TRUE), 2),
    median_rate = round(median(AnnualShoalingRate_ftperyr, na.rm = TRUE), 2),
    pct_positive = round(mean(AnnualShoalingRate_ftperyr > 0, na.rm = TRUE) * 100, 1)
  )
print(reach_thresholds)
```

## Correlation Matrix

```{r corrplot, fig.height=10}
### Select all numeric predictors, exclude annual shoaling rate since that's what
### were trying to model and use the cor function to make a correlation matrix
cor_matrix<-gage_CSAT_joined|>
  select(where(is.numeric),-AnnualShoalingRate_ftperyr)|>
  cor(use = "pairwise.complete.obs")
cor_matrix

### Select the highly correlated pairs and store in a dataframe
high_cor_pairs<- function(cor_mat, threshold = 0.7){
  cor_mat[lower.tri(cor_mat)] <- NA
  diag(cor_mat) <- NA

  high_cor <- which(abs(cor_mat) > threshold, arr.ind=TRUE)
  if (nrow(high_cor) > 0) {
    data.frame(
      var1 = rownames(cor_mat)[high_cor[,1]],
      var2 = rownames(cor_mat)[high_cor[,2]],
      correlation = cor_mat[high_cor]) |>
      arrange(desc(abs(correlation)))
} else {
  data.frame()
}
}

### Plot the corrplot 
library(corrplot)
corrplot(cor_matrix, tl.cex = 0.75, type = "upper", method = "color",
          order = "hclust", tl.col = "black")
```

## Create pool-level datasets

```{r}
### Pool statistics
pool_samples <- CSAT_data_cleaned |>
group_by(pool, river) |>
summarise(
  n_samples = n(),
  min_date = min(SurveyDateBefore),
  max_date = max(SurveyDateBefore),
  avg_shoaling = mean(AnnualShoalingRate_ftperyr, na.rm = TRUE),
  sd_shoaling = sd(AnnualShoalingRate_ftperyr, na.rm = TRUE),
  pct_positive = mean(AnnualShoalingRate_ftperyr > 0, na.rm = TRUE),
  .groups = "drop"
) |>
mutate(
  river_name = ifelse(river == "IL", "Illinois Waterway", "Mississippi River")
)

# Summarize gage information per pool
pool_gages <- gage_metadata |>
filter(!is.na(Pool)) |>
group_by(Pool, River) |>
summarise(
  n_gages_in_pool = n(),
  n_main_gages = sum(GageType == "Main"),
  n_trib_gages = sum(GageType == "Trib"),
  min_rm = min(RiverMile),
  max_rm = max(RiverMile),
  .groups = "drop"
)

# Join pool information
pool_info <- pool_samples |>
left_join(pool_gages, by = c("pool" = "Pool", "river" = "River"))|>
  mutate(center_rm = (min_rm + max_rm) / 2)  

cat("Pool Information Summary:\n")
print(pool_info |> select(pool, river_name, n_samples, n_gages_in_pool, pct_positive))

```

### Select gages upstream and within pools

```{r gage selection}
# Function to select gages relevant to a specific pool
select_pool_gages <- function(target_pool, target_river, metadata,
                             upstream_distance = 100,
                             include_tributaries = TRUE) {

# Get gages in the target pool
pool_gages_df <- metadata |>
  filter(Pool == target_pool, River == target_river)

if (nrow(pool_gages_df) == 0) return(NULL)

pool_max_rm <- max(pool_gages_df$RiverMile, na.rm = TRUE)
pool_min_rm <- min(pool_gages_df$RiverMile, na.rm = TRUE)

# Start with gages directly in the pool
selected <- pool_gages_df$Gage_ID

# Add upstream main channel gages
upstream_main <- metadata |>
  filter(
    River == target_river,
    GageType == "Main",
    RiverMile > pool_max_rm,
    RiverMile <= pool_max_rm + upstream_distance
  ) |>
  pull(Gage_ID)

selected <- unique(c(selected, upstream_main))

# Add tributary gages 
if (include_tributaries) {
  pool_tribs <- metadata |>
    filter(River == target_river, GageType == "Trib", Pool == target_pool) |>
    pull(Gage_ID)
  
  upstream_tribs <- metadata |>
    filter(
      River == target_river,
      GageType == "Trib",
      RiverMile > pool_max_rm,
      RiverMile <= pool_max_rm + upstream_distance
    ) |>
    pull(Gage_ID)
  
  selected <- unique(c(selected, pool_tribs, upstream_tribs))
}

return(selected)
}
```

### Assign gages to pools

```{r}
# Function to create complete mapping
create_pool_gage_mapping <- function(pool_info, metadata, upstream_distance = 100) {

  mapping <- list()
  
  for (i in 1:nrow(pool_info)) {
    pool <- pool_info$pool[i]
    river <- pool_info$river[i]
    key <- paste(river, pool, sep = "_") 
    
    gages <- select_pool_gages(
      target_pool = pool,
      target_river = river,
      metadata = metadata,
      upstream_distance = upstream_distance
    )
    
    mapping[[key]] <- list(
      pool = pool,
      river_code = river,
      river_filter = river,
      n_samples = pool_info$n_samples[i],
      selected_gages = gages,
      n_selected_gages = length(gages)
    )
  }
  
  return(mapping)
}
# Create mapping
pool_gage_mapping <- create_pool_gage_mapping(pool_info, gage_metadata)

# Display mapping summary
mapping_summary <- data.frame(
Key = names(pool_gage_mapping),
Pool = sapply(pool_gage_mapping, function(x) x$pool),
River = sapply(pool_gage_mapping, function(x) x$river_code),
N_Samples = sapply(pool_gage_mapping, function(x) x$n_samples),
N_Gages = sapply(pool_gage_mapping, function(x) x$n_selected_gages)
)


print(mapping_summary |> arrange(River, desc(N_Samples)))
```

## Principal Components Analysis

### Combined PCA

```{r Combined PCA}
colSums(is.na(gage_CSAT_joined))

 gage_survey_PCA <- gage_CSAT_joined |>
 select(where(is.numeric),
        -Year, -Month, -Day,-WeekOfYear, -SurveyDateBefore,-SurveyDateAfter,
        -rate_reliability,-reach, -NetVolumeChange_CY, -AnnualShoalingVolume_CYperyr, -AnnualShoalingRate_ftperyr, -SurveyOverlapArea_sqft, -SurveyOverlapPctReach)

# Remove rows with any missing values for PCA 
pca_data_complete <- gage_survey_PCA |>
  drop_na()

cat("Complete cases for PCA:", nrow(pca_data_complete), "out of", nrow(gage_CSAT_joined), "\n")
 
Comb_Viz <- gage_CSAT_joined |>
  dplyr::slice(as.numeric(rownames(pca_data_complete))) |>  
  select(SurveyDateBefore, Day, Month, WeekOfYear,
         river, pool, Season) |>
  mutate(
    season = factor(Season, levels = c("Winter", "Spring", "Summer", "Fall")),
    river_system = factor(river),
    pool_name = factor(pool),
    week_of_year = WeekOfYear)
  

Combined_PCA = prcomp(pca_data_complete, scale = T, center = T)

#Variance Explained
Combined_explained_variance <- summary(Combined_PCA)$importance[2, ] * 100
Combined_cumulative_variance <- summary(Combined_PCA)$importance[3, ] * 100
Combined_eigenvalues <- (Combined_PCA$sdev)^2
Combined_components_kaiser <- sum(Combined_eigenvalues > 1)

cat("\nCombined PCA Results:\n")
cat("  Variance explained (PC1-5):", round(Combined_explained_variance[1:5], 2), "%\n")
cat("  Cumulative variance (PC1-5):", round(Combined_cumulative_variance[1:5], 2), "%\n")
cat("  Components with eigenvalue > 1:", Combined_components_kaiser, "\n")

# Scree Plot
plot(Combined_PCA,  type="l", title = "Combined PCA Scree Plot")

# Biplot Function
plot_PCA_biplot <- function(River_df, River_Viz,RiverName) {
a<-autoplot(River_df,data = River_Viz, 
         color = 'season',loadings = TRUE,
             loadings.label = TRUE, # Show loading labels
             loadings.colour = "blue", # Color loading arrows
             loadings.label.colour = "darkblue")+
  ggtitle("Season")

b<-autoplot(River_df,data = River_Viz, 
         color = 'pool',loadings= TRUE,
             loadings.label = TRUE, # Show loading labels
             loadings.colour = "blue", # Color loading arrows
             loadings.label.colour = "darkblue")+
  labs(color = "Pool")+
  ggtitle(RiverName)+
    guides(color = guide_legend(ncol = 3))


c<-autoplot(River_df,data = River_Viz, 
         color = 'week_of_year',loadings = TRUE,
             loadings.label = TRUE, # Show loading labels
             loadings.colour = "blue", # Color loading arrows
             loadings.label.colour = "darkblue")+
        ggtitle("Week ")+
  labs(color = "Week of Year?")

library(patchwork)
combined <- a + b + c + plot_layout(ncol = 3) +
  plot_annotation(title = RiverName)

return(combined)
}

Combined_Biplots <- plot_PCA_biplot(Combined_PCA,Comb_Viz, "Combined Rivers")

Combined_Biplots

ggsave("./Output/PCA/Combined_PCA_Biplots.png", width = 18, height = 9)
```

## River-Specific PCA

```{r, fig.height=8}
IWW_complete<- gage_CSAT_joined |>
filter(river == "IL")|>
  select(any_of(IWW_gages), SurveyDateBefore, Season, Day, Month, WeekOfYear,
         river, pool) |>
  drop_na(any_of(IWW_gages)) 


IWW_pca_data <- IWW_complete|>
select(any_of(IWW_gages)) 

IWW_PCA <- prcomp(IWW_pca_data, scale = TRUE, center = TRUE)
IWW_explained_variance <- summary(IWW_PCA)$importance[2, ] * 100
IWW_cumulative_variance <- summary(IWW_PCA)$importance[3, ] * 100

# Mississippi PCA
Miss_complete <- gage_CSAT_joined |>
filter(river == "UM") |>
  select(any_of(Miss_gages), SurveyDateBefore, Season, Day, Month, WeekOfYear,
         river, pool) |>
    drop_na(any_of(Miss_gages)) 

Miss_pca_data <- Miss_complete|>
  select(any_of(Miss_gages))

Miss_PCA <- prcomp(Miss_pca_data, scale = TRUE, center = TRUE)
Miss_explained_variance <- summary(Miss_PCA)$importance[2, ] * 100
Miss_cumulative_variance <- summary(Miss_PCA)$importance[3, ] * 100

# Biplots of Miss and IL
IWW_Viz <-IWW_complete |>
  select(SurveyDateBefore, Season, Day, Month, WeekOfYear, river, pool) |>
  mutate(
    season = factor(Season, levels = c("Winter", "Spring", "Summer", "Fall")),
    river_system = factor(river),
    pool_name = factor(pool),
    week_of_year = WeekOfYear)

Miss_Viz <- Miss_complete |>
  select(SurveyDateBefore, Season, Day, Month, WeekOfYear, river, pool) |>
  mutate(
    season = factor(Season, levels = c("Winter", "Spring", "Summer", "Fall")),
    river_system = factor(river),
    pool_name = factor(pool),
    week_of_year = WeekOfYear)


IWW_Biplots <- plot_PCA_biplot(IWW_PCA,IWW_Viz, "Illinois Waterway")
IWW_Biplots[[2]]
ggsave("./Output/PCA/IWW_Season_PCA.png", width = 12, height = 6)
IWW_Biplots
ggsave("./Output/PCA/IWW_PCA_Biplots.png", width = 18, height = 9)

Miss_Biplots <- plot_PCA_biplot(Miss_PCA,Miss_Viz, "Mississippi River")
Miss_Biplots
ggsave("./Output/PCA/Miss_PCA_Biplots.png", width = 18, height = 9)
Miss_Biplots[[2]]
ggsave("./Output/PCA/Miss_Season_PCA.png", width = 12, height = 6)

# Comparison table
pca_comparison <- data.frame(
Analysis = c("IWW Only", "Mississippi Only", "Combined"),
Observations = c(nrow(IWW_pca_data), nrow(Miss_pca_data), nrow(pca_data_complete)),
Variables = c(ncol(IWW_pca_data), ncol(Miss_pca_data), ncol(pca_data_complete)),
PC1_Variance = c(round(IWW_explained_variance[1], 2),
                 round(Miss_explained_variance[1], 2),
                 round(Combined_explained_variance[1], 2)),
PC2_Variance = c(round(IWW_explained_variance[2], 2),
                 round(Miss_explained_variance[2], 2),
                 round(Combined_explained_variance[2], 2)),
PC1_PC2_Combined = c(round(IWW_explained_variance[1] + IWW_explained_variance[2], 2),
                     round(Miss_explained_variance[1] + Miss_explained_variance[2], 2),
                     round(Combined_explained_variance[1] + Combined_explained_variance[2], 2))
)

cat("\nPCA Comparison Summary:\n")
print(pca_comparison)

# Variance comparison plot
variance_data <- data.frame(
Component = rep(1:10, 3),
Variance = c(IWW_explained_variance[1:10],
             Miss_explained_variance[1:10],
             Combined_explained_variance[1:10]),
Cumulative = c(IWW_cumulative_variance[1:10],
               Miss_cumulative_variance[1:10],
               Combined_cumulative_variance[1:10]),
Analysis = rep(c("IWW Only", "Mississippi Only", "Combined"), each = 10)
)

p_var <- ggplot(variance_data, aes(x = Component, y = Variance, color = Analysis)) +
geom_line(linewidth = 1.2) +
geom_point(size = 3) +
scale_color_viridis_d() +
labs(title = "Variance Explained by Principal Components",
    x = "Principal Component", y = "Variance Explained (%)") +
theme_minimal()

p_cum <- ggplot(variance_data, aes(x = Component, y = Cumulative, color = Analysis)) +
geom_line(linewidth = 1.2) +
geom_point(size = 3) +
geom_hline(yintercept = c(80, 85, 90), linetype = "dashed", alpha = 0.5) +
scale_color_viridis_d() +
labs(title = "Cumulative Variance Explained",
    x = "Principal Component", y = "Cumulative Variance (%)") +
theme_minimal()

p_var / p_cum

ggsave("./Output/PCA/Variance_Comparison.png", width = 12, height = 10)

Miss_Iww_Biplot <-Miss_Biplots[[2]] + IWW_Biplots[[2]]+plot_layout(ncol = 2) +  plot_annotation(title = "PCA of Illinois Waterway and Mississippi River")
ggsave("./Output/PCA/Miss_IWW_Biplot.png", width =10, height = 4)
```

# Utility Functions

```{r temporal-splits-function}
create_temporal_splits <- function(data, train_prop = 0.70, val_prop = 0.15) {

# Ensure data is sorted by date
data <- data |> arrange(SurveyDateBefore)

n <- nrow(data)
train_end <- floor(n * train_prop)
val_end <- floor(n * (train_prop + val_prop))

splits <- list(
  train_idx = 1:train_end,
  val_idx = (train_end + 1):val_end,
  test_idx = (val_end + 1):n,
  train_dates = data$SurveyDateBefore[1:train_end],
  val_dates = data$SurveyDateBefore[(train_end + 1):val_end],
  test_dates = data$SurveyDateBefore[(val_end + 1):n]
)

cat(sprintf("Temporal splits:\n"))
cat(sprintf("  Training: %d samples (%.1f%%) - %s to %s\n",
            length(splits$train_idx), train_prop * 100,
            min(splits$train_dates), max(splits$train_dates)))
cat(sprintf("  Validation: %d samples (%.1f%%) - %s to %s\n",
            length(splits$val_idx), val_prop * 100,
            min(splits$val_dates), max(splits$val_dates)))
cat(sprintf("  Test: %d samples (%.1f%%) - %s to %s\n",
            length(splits$test_idx), (1 - train_prop - val_prop) * 100,
            min(splits$test_dates), max(splits$test_dates)))

return(splits)
}

# Create splits for each dataset
IWW_splits <- create_temporal_splits(IWW_data)
Miss_splits <- create_temporal_splits(Miss_data)
Combined_splits <- create_temporal_splits(gage_CSAT_joined)
```

## Binary Classification Function (Convert shoaling rates to dredge yes/no class)

```{r}
print(river_thresholds)
print(pool_thresholds)

IWW_thresh = river_thresholds |>
  filter(river == "IL")|>
  pull(mean_rate)
Miss_thresh = river_thresholds |>
  filter(river == "UM")|>
  pull(mean_rate)

calculate_classification_metrics <- function(actual, predicted, threshold) {

# Convert to binary
actual_bin <- ifelse(actual > threshold, "YES", "NO")
pred_bin <- ifelse(predicted > threshold, "YES", "NO")

# Confusion matrix components
TP <- sum(actual_bin == "YES" & pred_bin == "YES")
FP <- sum(actual_bin == "NO" & pred_bin == "YES")
FN <- sum(actual_bin == "YES" & pred_bin == "NO")
TN <- sum(actual_bin == "NO" & pred_bin == "NO")

# Metrics
accuracy <- (TP + TN) / (TP + FP + FN + TN)
precision <- ifelse((TP + FP) > 0, TP / (TP + FP), NA)
recall <- ifelse((TP + FN) > 0, TP / (TP + FN), NA)
f1 <- ifelse(!is.na(precision) & !is.na(recall) & (precision + recall) > 0,
             2 * precision * recall / (precision + recall), NA)

list(
  accuracy = accuracy,
  precision = precision,
  recall = recall,
  f1 = f1,
  confusion = table(Actual = actual_bin, Predicted = pred_bin)
)
}
```

# Baseline Models

```{r baseline-models}
create_baselines <- function(data, splits, dataset_name) {

train_data <- data[splits$train_idx, ]
test_data <- data[splits$test_idx, ]

# Persistence baseline (naive forecast)
persistence_pred <- c(
  tail(train_data$AnnualShoalingRate_ftperyr, 1),
  test_data$AnnualShoalingRate_ftperyr[-nrow(test_data)]
)
persistence_rmse <- sqrt(mean((persistence_pred - test_data$AnnualShoalingRate_ftperyr)^2))
persistence_mae <- mean(abs(persistence_pred - test_data$AnnualShoalingRate_ftperyr))

# Mean baseline
mean_pred <- rep(mean(train_data$AnnualShoalingRate_ftperyr, na.rm = TRUE), nrow(test_data))
mean_rmse <- sqrt(mean((mean_pred - test_data$AnnualShoalingRate_ftperyr)^2))
mean_mae <- mean(abs(mean_pred - test_data$AnnualShoalingRate_ftperyr))

# ARIMA baseline
train_ts <- ts(train_data$AnnualShoalingRate_ftperyr, frequency = 12)
arima_model <- auto.arima(train_ts, seasonal = TRUE, stepwise = TRUE,
                          approximation = FALSE, trace = FALSE)
arima_forecast <- forecast(arima_model, h = nrow(test_data))
arima_pred <- as.numeric(arima_forecast$mean)
arima_rmse <- sqrt(mean((arima_pred - test_data$AnnualShoalingRate_ftperyr)^2))
arima_mae <- mean(abs(arima_pred - test_data$AnnualShoalingRate_ftperyr))

results <- data.frame(
  Model = c("Persistence", "Mean", "ARIMA"),
  Test_RMSE = c(persistence_rmse, mean_rmse, arima_rmse),
  Test_MAE = c(persistence_mae, mean_mae, arima_mae),
  Dataset = dataset_name
) |>
arrange(Test_RMSE)

cat("\nBaseline Performance:\n")
print(results)

return(list(
  results = results,
  arima_model = arima_model,
  predictions = data.frame(
    actual = test_data$AnnualShoalingRate_ftperyr,
    persistence = persistence_pred,
    mean = mean_pred,
    arima = arima_pred,
    date = test_data$SurveyDateBefore
  )
))
}

# Create baselines
IWW_baselines <- create_baselines(IWW_data, IWW_splits, "Illinois Waterway")
Miss_baselines <- create_baselines(Miss_data, Miss_splits, "Mississippi River")
Combined_baselines <- create_baselines(gage_CSAT_joined |> mutate(pool = as.factor(pool)), Combined_splits, "Combined")
```

# xGBoost

## River Level xGBoost

### Model and Tuning Grid Set Up

```{r}
train_xgboost_temporal <- function(data, splits, gages_to_use, dataset_name, threshold) {

cat(sprintf("\n Training xGBoost: %s \n", dataset_name))

# Prepare features
xgb_features <- data |>
  select(AnnualShoalingRate_ftperyr, WeekOfYear, pool, any_of(gages_to_use)) |>
  drop_na()

n_samples <- nrow(xgb_features)
n_folds <- 5

# Create expanding window temporal folds
temporal_indices <- lapply(1:n_folds, function(i) {
  test_start <- floor(n_samples * i / (n_folds + 1))
  test_end <- floor(n_samples * (i + 1) / (n_folds + 1))
  list(train = 1:(test_start - 1), test = test_start:test_end)
})

temporal_indices <- temporal_indices[sapply(temporal_indices, function(x) length(x$train) > 50)]

# Training control
ctrl <- trainControl(
  method = "cv",
  number = length(temporal_indices),
  index = lapply(temporal_indices, `[[`, "train"),
  indexOut = lapply(temporal_indices, `[[`, "test"),
  verboseIter = FALSE,
  allowParallel = TRUE,
  savePredictions = "final"
)

# Tune grid
  tune_grid <- expand.grid(
    nrounds = c(100, 200),
    max_depth = c(4, 5),
    eta = c(0.05,0.1),
    gamma = c(0, 0.1),
    colsample_bytree = c(0.6, 0.8),
    min_child_weight = c(1, 3),
    subsample = c(0.8, 1.0)
  )
  11

cat(sprintf("Training with %d temporal CV folds...\n", length(temporal_indices)))

start_time <- Sys.time()
xgb_model <- caret::train(
  AnnualShoalingRate_ftperyr ~ .,
  data = xgb_features,
  method = "xgbTree",
  tuneGrid = tune_grid,
  trControl = ctrl,
  metric = "RMSE",
  verbose = FALSE
)
training_time <- difftime(Sys.time(), start_time, units = "mins")

cat(sprintf("Training time: %.2f minutes\n", as.numeric(training_time)))
cat(sprintf("Best CV RMSE: %.4f\n", min(xgb_model$results$RMSE)))
cat(sprintf("Best params: nrounds=%d, max_depth=%d, eta=%.3f\n",
            xgb_model$bestTune$nrounds,
            xgb_model$bestTune$max_depth,
            xgb_model$bestTune$eta))

# Test set evaluation
test_data <- data[splits$test_idx, ] |>
  select(AnnualShoalingRate_ftperyr, WeekOfYear, pool,
         any_of(gages_to_use), SurveyDateBefore) |>
  drop_na()

test_pred <- predict(xgb_model, newdata = test_data)
test_rmse <- sqrt(mean((test_pred - test_data$AnnualShoalingRate_ftperyr)^2))
test_mae <- mean(abs(test_pred - test_data$AnnualShoalingRate_ftperyr))
test_r2 <- cor(test_pred, test_data$AnnualShoalingRate_ftperyr)^2

# Classification metrics
class_metrics <- calculate_classification_metrics(
  test_data$AnnualShoalingRate_ftperyr, test_pred, threshold)

cat(sprintf("Test RMSE: %.4f, MAE: %.4f, RÂ²: %.4f\n", test_rmse, test_mae, test_r2))
cat(sprintf("Classification Accuracy: %.3f, F1: %.3f\n",
            class_metrics$accuracy, class_metrics$f1))

return(list(
  model = xgb_model,
  metrics = data.frame(
    Dataset = dataset_name,
    Model = "xGBoost",
    CV_RMSE = min(xgb_model$results$RMSE),
    Test_RMSE = test_rmse,
    Test_MAE = test_mae,
    Test_R2 = test_r2,
    Accuracy = class_metrics$accuracy,
    Precision = class_metrics$precision,
    Recall = class_metrics$recall,
    F1_Score = class_metrics$f1
  ),
  predictions = data.frame(
    actual = test_data$AnnualShoalingRate_ftperyr,
    predicted = test_pred,
    date = test_data$SurveyDateBefore
  ),
  confusion = class_metrics$confusion
))
}

```

### Train the models

```{r train-xgboost-river}
# Train river-level models
IWW_xgb <- train_xgboost_temporal(IWW_data, IWW_splits, IWW_gages, "Illinois Waterway", IWW_thresh)
Miss_xgb <- train_xgboost_temporal(Miss_data, Miss_splits, Miss_gages, "Mississippi River",Miss_thresh)
Combined_xgb <- train_xgboost_temporal(gage_CSAT_joined, Combined_splits, All_gages, "Combined", comb_thresh)
```

### Variable Importance

```{r}
# Extract and plot importance
iww_imp <- xgb.importance(model = IWW_xgb$model$finalModel)
miss_imp <- xgb.importance(model = Miss_xgb$model$finalModel)
combined_imp <- xgb.importance(model = Combined_xgb$model$finalModel)

cat("\nTop 10 Features - Illinois Waterway:\n")
print(head(iww_imp, 10))

cat("\nTop 10 Features - Mississippi River:\n")
print(head(miss_imp, 10))

# Save importance plots
png("./Output/xGBoost/IWW_Importance.png", width = 12, height = 8, units = "in", res = 300)
xgb.plot.importance(iww_imp[1:10, ], main = "Top 10 Features - Illinois Waterway")
dev.off()

png("./Output/xGBoost/Miss_Importance.png", width = 12, height = 8, units = "in", res = 300)
xgb.plot.importance(miss_imp[1:10, ], main = "Top 10 Features - Mississippi River")
dev.off()

png("./Output/xGBoost/Combined_Importance.png", width = 12, height = 8, units = "in", res = 300)
xgb.plot.importance(combined_imp[1:10, ], main = "Top 10 Features - Combined")
dev.off()

cat("\nImportance plots saved to ./Output/xGBoost/\n")
```

## Pool Level xGBoost

```{r}
train_pool_xgboost <- function(data, pool_name, river_code,
                              pool_gage_mapping, threshold, min_samples = 40) {

  key <- paste(river_code, pool_name, sep = "_")
  
  if (!key %in% names(pool_gage_mapping)) {
    return(list(pool = pool_name, river = river_code, status = "no_mapping", metrics = NULL))
  }
  
  pool_map <- pool_gage_mapping[[key]]
  river_filter <- pool_map$river_filter  # Now this exists
  selected_gages <- pool_map$selected_gages
  
  # Filter data for this pool
  pool_data <- data |>
    filter(pool == pool_name, river == river_filter) |>
    arrange(SurveyDateBefore)

if (nrow(pool_data) < min_samples) {
  return(list(pool = pool_name, river = river_code, status = "insufficient_samples",
              n = nrow(pool_data), metrics = NULL))
}

available_gages <- intersect(selected_gages, names(pool_data))

if (length(available_gages) < 2) {
  return(list(pool = pool_name, river = river_code, status = "insufficient_gages",
              n_gages = length(available_gages), metrics = NULL))
}

cat(sprintf("\n--- Pool %s (%s): %d samples, %d gages ---\n",
            pool_name, river_code, nrow(pool_data), length(available_gages)))

# Prepare features
features <- pool_data |>
  select(AnnualShoalingRate_ftperyr, WeekOfYear, DaysBetween, all_of(available_gages)) |>
  drop_na()

if (nrow(features) < 30) {
  return(list(pool = pool_name, river = river_code, status = "insufficient_complete",
              n_complete = nrow(features), metrics = NULL))
}

# Temporal split
n <- nrow(features)
train_end <- floor(n * 0.7)
test_start <- floor(n * 0.85) + 1

train_data <- features[1:train_end, ]
test_data <- features[test_start:n, ]

if (nrow(test_data) < 5) {
  return(list(pool = pool_name, river = river_code, status = "insufficient_test", metrics = NULL))
}

# Training
ctrl <- trainControl(method = "cv", number = 5, verboseIter = FALSE, allowParallel = TRUE)

tune_grid <- expand.grid(
  nrounds = c(50, 100, 150),
  max_depth = c(3, 4, 5, 6),
  eta = c(0.05, 0.1, 0.2),
  gamma = c(0, 0.1),
  colsample_bytree = c(0.7, 0.9),
  min_child_weight = c(1, 3),
  subsample = 0.8
)

model <- tryCatch({
  caret::train(
    AnnualShoalingRate_ftperyr ~ .,
    data = train_data,
    method = "xgbTree",
    tuneGrid = tune_grid,
    trControl = ctrl,
    metric = "RMSE",
    verbose = FALSE
  )
}, error = function(e) {
  cat(sprintf("  Training error: %s\n", e$message))
  return(NULL)
})

if (is.null(model)) {
  return(list(pool = pool_name, river = river_code, status = "training_failed", metrics = NULL))
}

# Evaluate
pred <- predict(model, test_data)
actual <- test_data$AnnualShoalingRate_ftperyr

rmse <- sqrt(mean((pred - actual)^2))
mae <- mean(abs(pred - actual))
r2 <- cor(pred, actual)^2

class_metrics <- calculate_classification_metrics(actual, pred, threshold)

cat(sprintf("  RMSE=%.3f, Accuracy=%.1f%%, F1=%.3f\n",
            rmse, class_metrics$accuracy * 100, class_metrics$f1))

return(list(
  pool = pool_name,
  river = river_code,
  status = "success",
  model = model,
  n_samples = nrow(pool_data),
  n_gages = length(available_gages),
  gages_used = available_gages,
  metrics = data.frame(
    Pool = pool_name,
    River = river_code,
    Model = "xGBoost",
    N_Samples = nrow(pool_data),
    N_Gages = length(available_gages),
    Test_RMSE = rmse,
    Test_MAE = mae,
    Test_R2 = r2,
    Accuracy = class_metrics$accuracy,
    Precision = class_metrics$precision,
    Recall = class_metrics$recall,
    F1_Score = class_metrics$f1
  ),
  predictions = data.frame(actual = actual, predicted = pred),
  confusion = class_metrics$confusion
))
}
```

### Run xGBoost at Pool Level

```{r}
viable_pools <- pool_info |>
  filter(n_samples >= 40) |>
  arrange(river, desc(n_samples))

cat(sprintf("Training pool-level xGBoost for %d viable pools\n", nrow(viable_pools)))

# Train models for each pool
pool_xgb_results <- list()

for (i in 1:nrow(viable_pools)) {
  pool_name <- viable_pools$pool[i]
  river_code <- viable_pools$river[i]  # This is "IL" or "UM"
  
  # Handle case where pool might not have threshold data
 pool_threshold <- pool_thresholds |>
    filter(pool == pool_name) |>
    pull(mean_rate)
  
  # Use river threshold as fallback if pool threshold missing
  if (length(pool_threshold) == 0) {
    pool_threshold <- river_thresholds |>
      filter(river == river_code) |>
      pull(mean_rate)
  }
  
  # Skip if still no threshold
 if (length(pool_threshold) == 0) {
    cat(sprintf("Skipping pool %s - no threshold available\n", pool_name))
    next
  }
  
  result <- train_pool_xgboost(
    data = gage_CSAT_joined,
    pool_name = pool_name,
    river_code = river_code,
    pool_gage_mapping = pool_gage_mapping,
    threshold = pool_threshold,
    min_samples = 40
  )
  
  key <- paste(river_code, pool_name, sep = "_")
  pool_xgb_results[[key]] <- result
}

# Compile results
pool_xgb_metrics <- do.call(rbind, lapply(pool_xgb_results, function(x) {
  if (!is.null(x$metrics)) x$metrics else NULL
}))

if (!is.null(pool_xgb_metrics)) {
  print(pool_xgb_metrics |> arrange(Test_RMSE))
}
```


# LSTM
LSTM Model developed using Claude AI on 11/01/2025. AI was used to build out the data preparation handling and workflow and to build model architecture to correctly handle temporal splits. Other reference materials include [R Bloggers Forecasting Sunspots](https://www.r-bloggers.com/2018/04/time-series-deep-learning-forecasting-sunspots-with-keras-stateful-lstm-in-r/)  and [Time Series Forecasting with RNN](https://blogs.rstudio.com/ai/posts/2017-12-20-time-series-forecasting-with-recurrent-neural-networks/){.uri} and [LSTM Time Series Prediction in R](http://datasideoflife.com/?p=1171)

## LSTM Data Preparation
```{r lstm-data-prep-function}
prepare_lstm_data_autoregressive <- function(data, 
                                              gages_to_use,
                                              sequence_length = 30,
                                              forecast_horizon = 40,
                                              train_prop = 0.70,
                                              val_prop = 0.15,
                                              dataset_name = "Dataset",
                                              shoaling_lags = c(1, 7, 14, 30),
                                              include_rolling_stats = TRUE) {
  
 cat(sprintf("\n%s\n", strrep("=", 70)))
 cat(sprintf("PREPARING LSTM DATA (AUTOREGRESSIVE): %s\n", dataset_name))
 cat(sprintf("%s\n", strrep("=", 70)))
  
 # Step 1: Sort by date and filter
 data <- data |>
   arrange(SurveyDateBefore) |>
   filter(!is.na(AnnualShoalingRate_ftperyr))
  
 original_n <- nrow(data)
 cat(sprintf("  Starting samples: %d\n", original_n))
  
 # Identify available gages
 available_gages <- intersect(gages_to_use, names(data))
  
 if (length(available_gages) < 2) {
   cat("  ERROR: Insufficient gage columns available\n")
   return(NULL)
 }
  
 cat(sprintf("  Available gages: %d of %d requested\n", 
             length(available_gages), length(gages_to_use)))
  
 # =========================================================================
 # STEP 2: CREATE AUTOREGRESSIVE FEATURES (KEY ADDITION - ARA APPROACH)
 # This is what makes the model work - past shoaling predicts future shoaling
 # =========================================================================
  
 cat("\n  Creating autoregressive shoaling features...\n")
  
 ar_feature_cols <- c()
  
 # Create lagged shoaling rates
 if (length(shoaling_lags) > 0) {
   for (lag in shoaling_lags) {
     lag_col <- paste0("shoaling_lag_", lag)
     data[[lag_col]] <- dplyr::lag(data$AnnualShoalingRate_ftperyr, lag)
     ar_feature_cols <- c(ar_feature_cols, lag_col)
     cat(sprintf("    + %s (lag %d observations)\n", lag_col, lag))
   }
 }
  
 # Create rolling statistics if requested
 if (include_rolling_stats) {
   data <- data |>
     mutate(
       # 7-day rolling mean of shoaling
       shoaling_roll_mean_7 = zoo::rollmean(AnnualShoalingRate_ftperyr, 7, 
                                             fill = NA, align = "right"),
       # 7-day rolling standard deviation (volatility)
       shoaling_roll_sd_7 = zoo::rollapply(AnnualShoalingRate_ftperyr, 7, 
                                            sd, fill = NA, align = "right"),
       # 14-day rolling mean
       shoaling_roll_mean_14 = zoo::rollmean(AnnualShoalingRate_ftperyr, 14, 
                                              fill = NA, align = "right"),
       # Trend: change from 7 days ago
       shoaling_trend_7 = AnnualShoalingRate_ftperyr - 
                          dplyr::lag(AnnualShoalingRate_ftperyr, 7),
       # Trend: change from 14 days ago  
       shoaling_trend_14 = AnnualShoalingRate_ftperyr - 
                           dplyr::lag(AnnualShoalingRate_ftperyr, 14),
       # Acceleration: is the trend increasing or decreasing?
       shoaling_accel = shoaling_trend_7 - dplyr::lag(shoaling_trend_7, 7)
     )
    
   rolling_cols <- c("shoaling_roll_mean_7", "shoaling_roll_sd_7",
                     "shoaling_roll_mean_14", "shoaling_trend_7", 
                     "shoaling_trend_14", "shoaling_accel")
   ar_feature_cols <- c(ar_feature_cols, rolling_cols)
    
   cat(sprintf("    + Rolling statistics: %s\n", paste(rolling_cols, collapse = ", ")))
 }
  
 cat(sprintf("  Total autoregressive features: %d\n", length(ar_feature_cols)))
  
 # Step 3: Select all features (gages + time + autoregressive)
 feature_cols <- c("AnnualShoalingRate_ftperyr", "WeekOfYear", "DaysBetween", 
                   available_gages, ar_feature_cols)
 feature_cols <- intersect(feature_cols, names(data))
  
 # Create feature matrix with complete cases only
 # (NAs from lagging will be removed here)
 feature_data <- data |>
   select(all_of(feature_cols), SurveyDateBefore) |>
   drop_na()
  
 n_samples <- nrow(feature_data)
 samples_lost <- original_n - n_samples
  
 cat(sprintf("\n  Samples after AR feature creation: %d (lost %d to lagging/NAs)\n",
             n_samples, samples_lost))
  
 min_required <- sequence_length + forecast_horizon + 50
  
 if (n_samples < min_required) {
   cat(sprintf("  ERROR: Insufficient samples (%d < %d required)\n", 
               n_samples, min_required))
   return(NULL)
 }
  
 # Summary of features
 n_gage_features <- length(available_gages)
 n_time_features <- sum(c("WeekOfYear", "DaysBetween") %in% feature_cols)
 n_ar_features <- length(ar_feature_cols)
 n_total_features <- length(feature_cols) - 1  # -1 for target
  
 cat(sprintf("\n  Feature breakdown:\n"))
 cat(sprintf("    Gage features: %d\n", n_gage_features))
 cat(sprintf("    Time features: %d\n", n_time_features))
 cat(sprintf("    Autoregressive features: %d\n", n_ar_features))
 cat(sprintf("    TOTAL input features: %d\n", n_total_features))
  
 # Step 4: Extract target and features
 target <- feature_data$AnnualShoalingRate_ftperyr
 dates <- feature_data$SurveyDateBefore
  
 features <- feature_data |>
   select(-SurveyDateBefore) |>
   as.matrix()
  
 # Step 5: CRITICAL - Proper normalization (from training data only)
 n <- nrow(features)
 train_end <- floor(n * train_prop)
 val_end <- floor(n * (train_prop + val_prop))
  
 # Calculate scaling parameters from training data only (prevents data leakage)
 train_features <- features[1:train_end, ]
  
 feature_means <- colMeans(train_features, na.rm = TRUE)
 feature_sds <- apply(train_features, 2, sd, na.rm = TRUE)
 feature_sds[feature_sds == 0] <- 1  # Prevent division by zero
  
 # Target scaling parameters
 target_mean <- mean(target[1:train_end], na.rm = TRUE)
 target_sd <- sd(target[1:train_end], na.rm = TRUE)
 if (target_sd == 0) target_sd <- 1
  
 # Apply normalization to ALL data using TRAINING parameters
 features_scaled <- scale(features, center = feature_means, scale = feature_sds)
 target_scaled <- (target - target_mean) / target_sd
  
 cat(sprintf("\n  Scaling parameters (from training data):\n"))
 cat(sprintf("    Target mean: %.3f, sd: %.3f\n", target_mean, target_sd))
 cat(sprintf("    Scaled target range: [%.3f, %.3f]\n", 
             min(target_scaled), max(target_scaled)))
  
 # Step 6: Create sequences for LSTM
 create_sequences <- function(features, target, seq_len, horizon) {
   n <- nrow(features)
   n_sequences <- n - seq_len - horizon + 1
    
   if (n_sequences <= 0) {
     return(list(X = NULL, y = NULL, dates = NULL))
   }
    
   n_features <- ncol(features)
    
   # Initialize arrays
   X <- array(0, dim = c(n_sequences, seq_len, n_features))
   y <- matrix(0, nrow = n_sequences, ncol = horizon)
   seq_dates <- vector("list", n_sequences)
    
   for (i in 1:n_sequences) {
     X[i, , ] <- features[i:(i + seq_len - 1), ]
     y[i, ] <- target[(i + seq_len):(i + seq_len + horizon - 1)]
     seq_dates[[i]] <- dates[i + seq_len]
   }
    
   return(list(X = X, y = y, dates = unlist(seq_dates)))
 }
  
 sequences <- create_sequences(features_scaled, target_scaled, 
                               sequence_length, forecast_horizon)
  
 if (is.null(sequences$X)) {
   cat("  ERROR: Failed to create sequences\n")
   return(NULL)
 }
  
 n_sequences <- dim(sequences$X)[1]
 cat(sprintf("\n  Created %d sequences (length=%d, horizon=%d)\n", 
             n_sequences, sequence_length, forecast_horizon))
  
 # Step 7: Temporal split (no shuffling - maintain time order)
 train_seq_end <- floor(n_sequences * train_prop)
 val_seq_end <- floor(n_sequences * (train_prop + val_prop))
  
 X_train <- sequences$X[1:train_seq_end, , , drop = FALSE]
 y_train <- sequences$y[1:train_seq_end, , drop = FALSE]
 dates_train <- sequences$dates[1:train_seq_end]
  
 X_val <- sequences$X[(train_seq_end + 1):val_seq_end, , , drop = FALSE]
 y_val <- sequences$y[(train_seq_end + 1):val_seq_end, , drop = FALSE]
 dates_val <- sequences$dates[(train_seq_end + 1):val_seq_end]
  
 X_test <- sequences$X[(val_seq_end + 1):n_sequences, , , drop = FALSE]
 y_test <- sequences$y[(val_seq_end + 1):n_sequences, , drop = FALSE]
 dates_test <- sequences$dates[(val_seq_end + 1):n_sequences]
  
 cat(sprintf("\n  Data splits:\n"))
 cat(sprintf("    Train: %d sequences (%.0f%%)\n", nrow(X_train), train_prop * 100))
 cat(sprintf("    Val:   %d sequences (%.0f%%)\n", nrow(X_val), val_prop * 100))
 cat(sprintf("    Test:  %d sequences (%.0f%%)\n", nrow(X_test), (1-train_prop-val_prop) * 100))
  
 cat(sprintf("\n  Tensor shapes:\n"))
 cat(sprintf("    X_train: [%d, %d, %d]\n", dim(X_train)[1], dim(X_train)[2], dim(X_train)[3]))
 cat(sprintf("    y_train: [%d, %d]\n", dim(y_train)[1], dim(y_train)[2]))
  
 return(list(
   X_train = X_train, y_train = y_train,
   X_val = X_val, y_val = y_val,
   X_test = X_test, y_test = y_test,
   dates_train = dates_train, dates_val = dates_val, dates_test = dates_test,
   target_mean = target_mean, target_sd = target_sd,
   feature_means = feature_means, feature_sds = feature_sds,
   sequence_length = sequence_length, forecast_horizon = forecast_horizon,
   n_features = ncol(features), n_train = nrow(X_train),
   n_val = nrow(X_val), n_test = nrow(X_test),
   feature_names = colnames(features),
   ar_features = ar_feature_cols,
   gage_features = available_gages,
   dataset_name = dataset_name
 ))
}
```

## LSTM Model Architecture Function
```{r}
build_lstm_model<- function(sequence_length, n_features, forecast_horizon) {
  inputs <- layer_input(shape = c(sequence_length, n_features))
  x <- inputs |>
    layer_lstm(units = 128, return_sequences = FALSE)
  outputs <- x |>
    layer_dense(units = forecast_horizon, activation = "linear")
  model <- keras_model(inputs, outputs)
  model <- keras3::compile(
    model,
    optimizer = optimizer_adam(learning_rate = 0.001),
    loss = "mse"
  )
  return(model)
}
```

## Comprehensive LSTM Training Function
```{r}
train_lstm <- function(lstm_data, dataset_name, threshold,
                              epochs = 100, batch_size = 8, patience = 20) {
  model <- build_lstm_model(
    sequence_length = lstm_data$sequence_length,
    n_features = lstm_data$n_features,
    forecast_horizon = lstm_data$forecast_horizon
  )
  
  callbacks <- list(
    callback_early_stopping(monitor = "val_loss", patience = patience, 
                            restore_best_weights = TRUE)
  )
  
  history <- keras3::fit(
    model,
    x = lstm_data$X_train, y = lstm_data$y_train,
    validation_data = list(lstm_data$X_val, lstm_data$y_val),
    epochs = epochs, batch_size = batch_size, callbacks = callbacks, verbose = 1
  )
  
  # Predict
  test_pred_scaled <- predict(model, lstm_data$X_test)
  test_pred <- test_pred_scaled * lstm_data$target_sd + lstm_data$target_mean
  test_actual <- lstm_data$y_test * lstm_data$target_sd + lstm_data$target_mean
  
  # === REGRESSION METRICS (Required by Proposal Objective 5) ===
  rmse <- sqrt(mean((test_pred[, 1] - test_actual[, 1])^2))
  mae <- mean(abs(test_pred[, 1] - test_actual[, 1]))
  correlation <- cor(test_pred[, 1], test_actual[, 1])
  variance_ratio <- var(test_pred[, 1]) / var(test_actual[, 1])
  
  # === CLASSIFICATION METRICS ===
  actual_class <- ifelse(test_actual[, 1] > threshold, "YES", "NO")
  pred_class <- ifelse(test_pred[, 1] > threshold, "YES", "NO")
  confusion <- table(Actual = actual_class, Predicted = pred_class)
  
  TP <- sum(actual_class == "YES" & pred_class == "YES")
  FP <- sum(actual_class == "NO" & pred_class == "YES")
  FN <- sum(actual_class == "YES" & pred_class == "NO")
  TN <- sum(actual_class == "NO" & pred_class == "NO")
  
  accuracy <- (TP + TN) / length(actual_class)
  precision <- ifelse((TP + FP) > 0, TP / (TP + FP), NA)
  recall <- ifelse((TP + FN) > 0, TP / (TP + FN), NA)
  f1 <- ifelse(!is.na(precision) & !is.na(recall) & (precision + recall) > 0,
               2 * precision * recall / (precision + recall), NA)
  
  # === HORIZON METRICS ===
  horizons <- c(1, 7, 14, 21, 30, 40)
  horizons <- horizons[horizons <= lstm_data$forecast_horizon]
  
  horizon_metrics <- data.frame(
    Horizon = horizons,
    RMSE = sapply(horizons, function(h) sqrt(mean((test_pred[, h] - test_actual[, h])^2))),
    MAE = sapply(horizons, function(h) mean(abs(test_pred[, h] - test_actual[, h]))),
    Correlation = sapply(horizons, function(h) cor(test_pred[, h], test_actual[, h]))
  )
  
  # === DIAGNOSTIC STATUS ===
  status <- ifelse(variance_ratio >= 0.5 & correlation >= 0.3, "HEALTHY", "PROBLEMATIC")
  
  cat(sprintf("\n=== %s LSTM Results ===\n", dataset_name))
  cat(sprintf("  RMSE: %.3f | MAE: %.3f | R: %.3f\n", rmse, mae, correlation))
  cat(sprintf("  Variance Ratio: %.3f | Status: %s\n", variance_ratio, status))
  cat(sprintf("  Accuracy: %.1f%% | F1: %.3f\n", accuracy * 100, f1))
  
  # Return comprehensive results
  list(
    model = model,
    history = history,
    test_actual = test_actual,
    test_pred = test_pred,
    test_dates = lstm_data$dates_test,
    confusion = confusion,
    # Regression metrics
    rmse = rmse,
    mae = mae,
    correlation = correlation,
    variance_ratio = variance_ratio,
    # Classification metrics
    accuracy = accuracy,
    precision = precision,
    recall = recall,
    f1 = f1,
    # Diagnostic
    status = status,
    # Horizon analysis
    horizon_metrics = horizon_metrics,
    # Metadata
    dataset_name = dataset_name,
    n_test = nrow(test_pred)
  )
}
```

## LSTM Validation Plots Function

```{r lstm-validation-plots}
plot_pred_vs_actual <- function(test_dates, test_actual, test_pred, dataset_name) {
  df <- data.frame(
    date = as.Date(test_dates),
    actual = test_actual[, 1],
    predicted = test_pred[, 1]
  )
  ggplot(df, aes(x = date)) +
    geom_line(aes(y = actual, color = "Actual"), size = 1) +
    geom_line(aes(y = predicted, color = "Predicted"), size = 1) +
    labs(title = paste(dataset_name, "- LSTM: Predicted vs Actual Shoaling Rate"),
         x = "Date", y = "Shoaling Rate (ft/yr)", color = "") +
    scale_color_manual(values = c("Actual" = "#2166AC", "Predicted" = "#D6604D"))+
    theme_minimal()
}
```

## River Level LSTM
```{r}
# Prepare data (Illinois Waterway example)
# Add a 7-day rolling mean of shoaling rate to your river-level datasets
IWW_data_rolld <- IWW_data |>
  arrange(SurveyDateBefore) |>
  mutate(AnnualShoalingRate_ftperyr = zoo::rollmean(AnnualShoalingRate_ftperyr, 7, fill = NA, align = "right"))


Miss_data_rolld <- Miss_data |>
  arrange(SurveyDateBefore) |>
  mutate(AnnualShoalingRate_ftperyr = zoo::rollmean(AnnualShoalingRate_ftperyr, 7, fill = NA, align = "right"))

Combined_data_rolld <- gage_CSAT_joined |>
  arrange(SurveyDateBefore) |>
  mutate(
    AnnualShoalingRate_ftperyr = zoo::rollmean(AnnualShoalingRate_ftperyr, 7, fill = NA, align = "right"),
    pool = as.factor(pool)
  )

IWW_lstm_data <- prepare_lstm_data_autoregressive(
  data = IWW_data_rolld,
  gages_to_use = IWW_gages,
  sequence_length = 30,
  forecast_horizon = 40,
  dataset_name = "Illinois Waterway",
  shoaling_lags = c(1, 7, 14, 30),
  include_rolling_stats = TRUE
)
library(keras)
# Train and evaluate
IWW_lstm_result <- train_lstm(
  lstm_data = IWW_lstm_data,
  dataset_name = "Illinois Waterway",
  threshold = IWW_thresh
)

Miss_lstm_data <- prepare_lstm_data_autoregressive(
  data = Miss_data_rolld,
  gages_to_use = Miss_gages,
  sequence_length = 30,
  forecast_horizon = 40,
  dataset_name = "Mississippi River",
  shoaling_lags = c(1, 7, 14, 30),
  include_rolling_stats = TRUE
)

# Train and evaluate
Miss_lstm_result <- train_lstm(
  lstm_data = Miss_lstm_data,
  dataset_name = "Mississippi River",
  threshold = Miss_thresh
)

Combined_lstm_data <- prepare_lstm_data_autoregressive(
  data = Combined_data_rolld,
  gages_to_use = All_gages,
  sequence_length = 30,
  forecast_horizon = 40,
  dataset_name = "Combined",
  shoaling_lags = c(1, 7, 14, 30),
  include_rolling_stats = TRUE
)

# Train and evaluate
Combined_lstm_result <- train_lstm(
  lstm_data = Combined_lstm_data,
  dataset_name = "Combined",
  threshold = comb_thresh
)


# Plot
iww_lstm_plot<-plot_pred_vs_actual(
  test_dates = IWW_lstm_result$test_dates,
  test_actual = IWW_lstm_result$test_actual,
  test_pred = IWW_lstm_result$test_pred,
  dataset_name = "Illinois Waterway"
)
ggsave("./Output/LSTM/IWW_LSTM_Pred_vs_Actual.png", iww_lstm_plot, width = 12, height = 8)

persistence_pred <- c(NA, head(IWW_lstm_result$test_actual[,1], -1))

test_dates = IWW_lstm_result$test_dates
  test_actual = IWW_lstm_result$test_actual
  test_pred = IWW_lstm_result$test_pred


plot_df <- data.frame(
  date = as.Date(IWW_lstm_result$test_dates),
  actual = IWW_lstm_result$test_actual[,1],
  predicted = IWW_lstm_result$test_pred[,1],
  persistence = persistence_pred
)

# Pivot longer for ggplot
library(tidyr)
plot_long <- plot_df |>
  pivot_longer(cols = c(actual, predicted, persistence), 
               names_to = "type", values_to = "shoaling_rate")

# Plot
ggplot(plot_long, aes(x = date, y = shoaling_rate, color = type)) +
  geom_line(size = 1) +
  labs(title = "IWW - LSTM: Predicted vs Actual Shoaling Rate",
       x = "Date", y = "Shoaling Rate (ft/yr)", color = "") +
  scale_color_manual(values = c("actual" = "#2166AC", 
                                "predicted" = "#D6604D",
                                "persistence" = "forestgreen"),
                     labels = c("Actual", "Predicted", "Persistence")) +
  theme_minimal()

miss_lstm_plot<-plot_pred_vs_actual(
  test_dates = Miss_lstm_result$test_dates,
  test_actual = Miss_lstm_result$test_actual,
  test_pred = Miss_lstm_result$test_pred,
  dataset_name = "Mississippi River"
)
ggsave("./Output/LSTM/Miss_LSTM_Pred_vs_Actual.png", miss_lstm_plot, width = 12, height = 8)

combined_lstm_plot <- plot_pred_vs_actual(
  test_dates = Combined_lstm_result$test_dates,
  test_actual = Combined_lstm_result$test_actual,
  test_pred = Combined_lstm_result$test_pred,
  dataset_name = "Combined"
)
ggsave("./Output/LSTM/Combined_LSTM_Pred_vs_Actual.png", combined_lstm_plot, width = 12, height = 8)


```

## Pool level LSTM
```{r}
pool_lstm_results <- list()
for (i in 1:nrow(viable_pools)) {
  pool_name <- viable_pools$pool[i]
  river_code <- viable_pools$river[i]
  key <- paste(river_code, pool_name, sep = "_")
  selected_gages <- pool_gage_mapping[[key]]$selected_gages
  
  # Apply 7-day rolling mean to pool data (FIXED)
  pool_data <- gage_CSAT_joined |>
    filter(pool == pool_name, river == river_code) |>
    arrange(SurveyDateBefore) |>
    mutate(AnnualShoalingRate_ftperyr = zoo::rollmean(AnnualShoalingRate_ftperyr, 7, fill = NA, align = "right"))
  
  pool_threshold <- pool_thresholds |>
    filter(pool == pool_name) |>
    pull(mean_rate)
  if (length(pool_threshold) == 0) {
    pool_threshold <- ifelse(river_code == "IL", IWW_thresh, Miss_thresh)
  }
  lstm_data <- prepare_lstm_data_autoregressive(
    data = pool_data,
    gages_to_use = selected_gages,
    sequence_length = 30,
    forecast_horizon = 40,
    dataset_name = paste("Pool", pool_name),
    shoaling_lags = c(1, 7, 14),
    include_rolling_stats = TRUE
  )
  if (!is.null(lstm_data)) {
    result <- train_lstm(
      lstm_data = lstm_data,
      dataset_name = paste("Pool", pool_name),
      threshold = pool_threshold
    )
    pool_lstm_results[[key]] <- result
    p_pool <- plot_pred_vs_actual(
      test_dates = result$test_dates,
      test_actual = result$test_actual,
      test_pred = result$test_pred,
      dataset_name = paste("Pool", pool_name)
    )
    ggsave(paste0("./Output/LSTM/Pool_", pool_name, "_LSTM_Pred_vs_Actual.png"), p_pool, width = 12, height = 8)
  }
}

```

### Full Results for LSTM
```{r}
# Compute comprehensive metrics
compute_lstm_metrics <- function(result, threshold, dataset_name) {
  actual_1step <- result$test_actual[, 1]
  pred_1step <- result$test_pred[, 1]
  
  # Regression metrics
  rmse <- sqrt(mean((pred_1step - actual_1step)^2))
  mae <- mean(abs(pred_1step - actual_1step))
  correlation <- cor(pred_1step, actual_1step)
  
  # CRITICAL: Variance ratio diagnostic
  variance_ratio <- var(pred_1step) / var(actual_1step)
  
  # Classification
  actual_class <- ifelse(actual_1step > threshold, "YES", "NO")
  pred_class <- ifelse(pred_1step > threshold, "YES", "NO")
  
  TP <- sum(actual_class == "YES" & pred_class == "YES")
  FP <- sum(actual_class == "NO" & pred_class == "YES")
  FN <- sum(actual_class == "YES" & pred_class == "NO")
  TN <- sum(actual_class == "NO" & pred_class == "NO")
  
  accuracy <- (TP + TN) / length(actual_class)
  precision <- ifelse((TP + FP) > 0, TP / (TP + FP), NA)
  recall <- ifelse((TP + FN) > 0, TP / (TP + FN), NA)
  f1 <- ifelse(!is.na(precision) & !is.na(recall) & (precision + recall) > 0,
               2 * precision * recall / (precision + recall), NA)
  
  # Status check
  status <- ifelse(variance_ratio >= 0.5 & correlation >= 0.3, "HEALTHY", "PROBLEMATIC")
  
  data.frame(
    Dataset = dataset_name,
    RMSE = round(rmse, 3),
    MAE = round(mae, 3),
    Correlation = round(correlation, 3),
    Variance_Ratio = round(variance_ratio, 3),
    Accuracy = round(accuracy, 3),
    Precision = round(precision, 3),
    Recall = round(recall, 3),
    F1 = round(f1, 3),
    Status = status
  )
}

# Generate results for both rivers
lstm_river_results <- bind_rows(
  compute_lstm_metrics(IWW_lstm_result, IWW_thresh, "Illinois Waterway"),
  compute_lstm_metrics(Miss_lstm_result, Miss_thresh, "Mississippi River"),
  compute_lstm_metrics(Combined_lstm_result, comb_thresh, "Combined")
)

cat("\n=== LSTM RIVER-LEVEL RESULTS ===\n")
print(lstm_river_results)

# Save results
write_csv(lstm_river_results, "./Output/LSTM/River_Level_Results.csv")

# Key finding
cat("\n=== DIAGNOSTIC STATUS ===\n")
for (i in 1:nrow(lstm_river_results)) {
  cat(sprintf("%s: %s (Variance Ratio: %.3f)\n",
              lstm_river_results$Dataset[i],
              lstm_river_results$Status[i],
              lstm_river_results$Variance_Ratio[i]))
}
```



# Compile Model Results 
```{r compile-results}
# --- River-Level Regression Results ---
river_regression <- bind_rows(
  # Baselines
  IWW_baselines$results |> mutate(River = "Illinois Waterway", Type = "Baseline"),
  Miss_baselines$results |> mutate(River = "Mississippi River", Type = "Baseline"),
  Combined_baselines$results |> mutate(River = "Combined", Type = "Baseline"),
  
  # xGBoost
  data.frame(
    Model = "xGBoost",
    Test_RMSE = IWW_xgb$metrics$Test_RMSE,
    Test_MAE = IWW_xgb$metrics$Test_MAE,
    Dataset = "Illinois Waterway",
    River = "Illinois Waterway",
    Type = "ML"
  ),
  data.frame(
    Model = "xGBoost",
    Test_RMSE = Miss_xgb$metrics$Test_RMSE,
    Test_MAE = Miss_xgb$metrics$Test_MAE,
    Dataset = "Mississippi River",
    River = "Mississippi River",
    Type = "ML"
  ),
  data.frame(
    Model = "xGBoost",
    Test_RMSE = Combined_xgb$metrics$Test_RMSE,
    Test_MAE = Combined_xgb$metrics$Test_MAE,
    Dataset = "Combined",
    River = "Combined",
    Type = "ML"
  ),
  
  # LSTM - IWW
  data.frame(
    Model = "LSTM",
    Test_RMSE = IWW_lstm_result$rmse,
    Test_MAE = IWW_lstm_result$mae,
    Dataset = "Illinois Waterway",
    River = "Illinois Waterway",
    Type = "ML"
  ),
  # LSTM - Mississippi
  data.frame(
    Model = "LSTM",
    Test_RMSE = Miss_lstm_result$rmse,
    Test_MAE = Miss_lstm_result$mae,
    Dataset = "Mississippi River",
    River = "Mississippi River",
    Type = "ML"
  ),
  # LSTM - Combined
  data.frame(
    Model = "LSTM",
    Test_RMSE = Combined_lstm_result$rmse,
    Test_MAE = Combined_lstm_result$mae,
    Dataset = "Combined",
    River = "Combined",
    Type = "ML"
  )
)

# --- Classification Results ---
classification_results <- bind_rows(
  # xGBoost
  IWW_xgb$metrics |> 
    select(Dataset, Accuracy, Precision, Recall, F1_Score) |> 
    mutate(Model = "xGBoost"),
  Miss_xgb$metrics |> 
    select(Dataset, Accuracy, Precision, Recall, F1_Score) |> 
    mutate(Model = "xGBoost"),
  Combined_xgb$metrics |> 
    select(Dataset, Accuracy, Precision, Recall, F1_Score) |> 
    mutate(Model = "xGBoost"),
  
  # LSTM - IWW
  data.frame(
    Dataset = "Illinois Waterway",
    Accuracy = IWW_lstm_result$accuracy,
    Precision = IWW_lstm_result$precision,
    Recall = IWW_lstm_result$recall,
    F1_Score = IWW_lstm_result$f1,
    Model = "LSTM"
  ),
  # LSTM - Mississippi
  data.frame(
    Dataset = "Mississippi River",
    Accuracy = Miss_lstm_result$accuracy,
    Precision = Miss_lstm_result$precision,
    Recall = Miss_lstm_result$recall,
    F1_Score = Miss_lstm_result$f1,
    Model = "LSTM"
  ),
  # LSTM - Combined
  data.frame(
    Dataset = "Combined",
    Accuracy = Combined_lstm_result$accuracy,
    Precision = Combined_lstm_result$precision,
    Recall = Combined_lstm_result$recall,
    F1_Score = Combined_lstm_result$f1,
    Model = "LSTM"
  )
)

cat("=== River-Level Regression Results ===\n")
print(river_regression |> arrange(River, Test_RMSE))

cat("\n=== Classification Results ===\n")
print(classification_results |> arrange(Dataset, desc(F1_Score)))
```

# Model Performance Summary
```{r performance-summary}
for (river in c("Illinois Waterway", "Mississippi River", "Combined")) {

  cat(sprintf("â %s:\n", river))
  
  river_data <- river_regression |>
    filter(River == river) |>
    arrange(Test_RMSE)
  
  best_baseline <- river_data |> filter(Type == "Baseline") |> dplyr::slice(1)
  best_ml <- river_data |> filter(Type == "ML") |> dplyr::slice(1)
  
  if (nrow(best_ml) > 0 && nrow(best_baseline) > 0) {
    improvement <- (best_baseline$Test_RMSE - best_ml$Test_RMSE) / best_baseline$Test_RMSE * 100
    
    cat(sprintf("â   Best Baseline: %s (RMSE: %.4f)\n", best_baseline$Model, best_baseline$Test_RMSE))
    cat(sprintf("â   Best ML Model: %s (RMSE: %.4f)\n", best_ml$Model, best_ml$Test_RMSE))
    cat(sprintf("â   Improvement: %.1f%%\n", improvement))
  }
  
  cat("â\n")
}


```
#Pool Level Results
```{r}
# ============================================================================
# POOL-LEVEL MODEL RESULTS AND COMPARISONS
# ============================================================================

# ----------------------------------------------------------------------------
# 1. POOL XGBOOST SUMMARY
# ----------------------------------------------------------------------------

cat("\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n")
cat("â                    POOL-LEVEL XGBOOST RESULTS                          â\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n\n")

if (!is.null(pool_xgb_metrics) && nrow(pool_xgb_metrics) > 0) {
  
  # Summary statistics by river
  pool_xgb_summary <- pool_xgb_metrics |>
    group_by(River) |>
    summarise(
      N_Pools = n(),
      Avg_RMSE = round(mean(Test_RMSE, na.rm = TRUE), 3),
      Min_RMSE = round(min(Test_RMSE, na.rm = TRUE), 3),
      Max_RMSE = round(max(Test_RMSE, na.rm = TRUE), 3),
      Avg_R2 = round(mean(Test_R2, na.rm = TRUE), 3),
      Avg_Accuracy = round(mean(Accuracy, na.rm = TRUE) * 100, 1),
      Avg_F1 = round(mean(F1_Score, na.rm = TRUE), 3),
      .groups = "drop"
    )
  
  cat("=== Summary by River System ===\n")
  print(pool_xgb_summary)
  
  # Best performing pools
  cat("\n=== Top 5 Best Performing Pools (by RMSE) ===\n")
  best_pools <- pool_xgb_metrics |>
    arrange(Test_RMSE) |>
    head(5) |>
    select(Pool, River, N_Samples, Test_RMSE, Test_R2, Accuracy, F1_Score) |>
    mutate(
      Test_RMSE = round(Test_RMSE, 3),
      Test_R2 = round(Test_R2, 3),
      Accuracy = round(Accuracy * 100, 1),
      F1_Score = round(F1_Score, 3)
    )
  print(best_pools)
  
  # Worst performing pools
  cat("\n=== Top 5 Worst Performing Pools (by RMSE) ===\n")
  worst_pools <- pool_xgb_metrics |>
    arrange(desc(Test_RMSE)) |>
    head(5) |>
    select(Pool, River, N_Samples, Test_RMSE, Test_R2, Accuracy, F1_Score) |>
    mutate(
      Test_RMSE = round(Test_RMSE, 3),
      Test_R2 = round(Test_R2, 3),
      Accuracy = round(Accuracy * 100, 1),
      F1_Score = round(F1_Score, 3)
    )
  print(worst_pools)
  
  # Classification performance
  cat("\n=== Classification Performance by Pool ===\n")
  class_summary <- pool_xgb_metrics |>
    arrange(desc(F1_Score)) |>
    select(Pool, River, Accuracy, Precision, Recall, F1_Score) |>
    mutate(
      Accuracy = round(Accuracy * 100, 1),
      Precision = round(Precision, 3),
      Recall = round(Recall, 3),
      F1_Score = round(F1_Score, 3)
    )
  print(class_summary)
  
} else {
  cat("No pool-level xGBoost results available\n")
}

# ----------------------------------------------------------------------------
# 2. POOL LSTM SUMMARY (if available)
# ----------------------------------------------------------------------------

cat("\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n")
cat("â                    POOL-LEVEL LSTM RESULTS                             â\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n\n")

if (!is.null(pool_lstm_metrics) && nrow(pool_lstm_metrics) > 0) {
  
  pool_lstm_summary <- pool_lstm_metrics |>
    group_by(River) |>
    summarise(
      N_Pools = n(),
      Avg_RMSE_1Step = round(mean(Test_RMSE_1Step, na.rm = TRUE), 3),
      Min_RMSE_1Step = round(min(Test_RMSE_1Step, na.rm = TRUE), 3),
      Max_RMSE_1Step = round(max(Test_RMSE_1Step, na.rm = TRUE), 3),
      Avg_Accuracy = round(mean(Accuracy, na.rm = TRUE) * 100, 1),
      Avg_F1 = round(mean(F1_Score, na.rm = TRUE), 3),
      .groups = "drop"
    )
  
  cat("=== LSTM Summary by River System ===\n")
  print(pool_lstm_summary)
  
  cat("\n=== Pool LSTM Detailed Results ===\n")
  print(pool_lstm_metrics |> 
          select(Pool, River, Test_RMSE_1Step, Accuracy, F1_Score) |>
          mutate(
            Test_RMSE_1Step = round(Test_RMSE_1Step, 3),
            Accuracy = round(Accuracy * 100, 1),
            F1_Score = round(F1_Score, 3)
          ) |>
          arrange(Test_RMSE_1Step))
  
} else {
  cat("No pool-level LSTM results available (requires >= 150 samples per pool)\n")
}

# ----------------------------------------------------------------------------
# 3. POOL vs RIVER-LEVEL COMPARISON
# ----------------------------------------------------------------------------

cat("\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n")
cat("â                 POOL-LEVEL vs RIVER-LEVEL COMPARISON                   â\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n\n")

# Create comparison table
comparison_data <- data.frame(
  Level = c("River-Level", "River-Level", "Pool-Level Avg", "Pool-Level Avg"),
  River = c("Illinois Waterway", "Mississippi River", "Illinois Waterway", "Mississippi River"),
  Model = rep("xGBoost", 4),
  RMSE = c(
    IWW_xgb$metrics$Test_RMSE,
    Miss_xgb$metrics$Test_RMSE,
    ifelse(!is.null(pool_xgb_metrics), 
           mean(pool_xgb_metrics$Test_RMSE[pool_xgb_metrics$River == "IL"], na.rm = TRUE), 
           NA),
    ifelse(!is.null(pool_xgb_metrics), 
           mean(pool_xgb_metrics$Test_RMSE[pool_xgb_metrics$River == "UM"], na.rm = TRUE), 
           NA)
  ),
  Accuracy = c(
    IWW_xgb$metrics$Accuracy,
    Miss_xgb$metrics$Accuracy,
    ifelse(!is.null(pool_xgb_metrics), 
           mean(pool_xgb_metrics$Accuracy[pool_xgb_metrics$River == "IL"], na.rm = TRUE), 
           NA),
    ifelse(!is.null(pool_xgb_metrics), 
           mean(pool_xgb_metrics$Accuracy[pool_xgb_metrics$River == "UM"], na.rm = TRUE), 
           NA)
  )
) |>
  mutate(
    RMSE = round(RMSE, 3),
    Accuracy = round(Accuracy * 100, 1)
  )

cat("=== xGBoost: River-Level vs Pool-Level ===\n")
print(comparison_data)

# Calculate if pool-level is better or worse
if (!is.null(pool_xgb_metrics)) {
  cat("\n=== Performance Difference (Pool - River) ===\n")
  
  iww_diff <- mean(pool_xgb_metrics$Test_RMSE[pool_xgb_metrics$River == "IL"], na.rm = TRUE) - 
              IWW_xgb$metrics$Test_RMSE
  miss_diff <- mean(pool_xgb_metrics$Test_RMSE[pool_xgb_metrics$River == "UM"], na.rm = TRUE) - 
               Miss_xgb$metrics$Test_RMSE
  
  cat(sprintf("  Illinois Waterway: %+.3f RMSE (%s)\n", 
              iww_diff, 
              ifelse(iww_diff < 0, "Pool-level BETTER", "River-level BETTER")))
  cat(sprintf("  Mississippi River: %+.3f RMSE (%s)\n", 
              miss_diff, 
              ifelse(miss_diff < 0, "Pool-level BETTER", "River-level BETTER")))
}

# ----------------------------------------------------------------------------
# 4. VISUALIZATIONS
# ----------------------------------------------------------------------------

# Plot 1: Pool RMSE Distribution by River
if (!is.null(pool_xgb_metrics) && nrow(pool_xgb_metrics) > 0) {
  
  p_pool_rmse <- pool_xgb_metrics |>
    mutate(river_label = ifelse(River == "IL", "Illinois Waterway", "Mississippi River")) |>
    ggplot(aes(x = reorder(Pool, Test_RMSE), y = Test_RMSE, fill = river_label)) +
    geom_col(alpha = 0.8) +
    geom_hline(data = data.frame(
      river_label = c("Illinois Waterway", "Mississippi River"),
      yint = c(IWW_xgb$metrics$Test_RMSE, Miss_xgb$metrics$Test_RMSE)
    ), aes(yintercept = yint), linetype = "dashed", color = "red", linewidth = 1) +
    facet_wrap(~river_label, scales = "free_x") +
    scale_fill_manual(values = c("Illinois Waterway" = "orange", "Mississippi River" = "dodgerblue")) +
    labs(
      title = "Pool-Level xGBoost RMSE Performance",
      subtitle = "Dashed line = River-level model RMSE",
      x = "Pool",
      y = "Test RMSE (ft/yr)",
      fill = "River"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"
    )
  
  print(p_pool_rmse)
  ggsave("./Output/Pool_Models/Pool_RMSE_Comparison.png", width = 14, height = 8)
  
  # Plot 2: Pool Classification Accuracy
  p_pool_accuracy <- pool_xgb_metrics |>
    mutate(river_label = ifelse(River == "IL", "Illinois Waterway", "Mississippi River")) |>
    ggplot(aes(x = reorder(Pool, Accuracy), y = Accuracy * 100, fill = river_label)) +
    geom_col(alpha = 0.8) +
    geom_hline(yintercept = 50, linetype = "dashed", color = "gray50") +
    facet_wrap(~river_label, scales = "free_x") +
    scale_fill_manual(values = c("Illinois Waterway" = "orange", "Mississippi River" = "dodgerblue")) +
    labs(
      title = "Pool-Level Classification Accuracy",
      subtitle = "Dashed line = 50% (random baseline)",
      x = "Pool",
      y = "Accuracy (%)",
      fill = "River"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      legend.position = "none"
    )
  
  print(p_pool_accuracy)
  ggsave("./Output/Pool_Models/Pool_Accuracy_Comparison.png", width = 14, height = 8)
  
  # Plot 3: Sample Size vs Performance
  p_samples_vs_rmse <- pool_xgb_metrics |>
    mutate(river_label = ifelse(River == "IL", "Illinois Waterway", "Mississippi River")) |>
    ggplot(aes(x = N_Samples, y = Test_RMSE, color = river_label)) +
    geom_point(size = 4, alpha = 0.7) +
    geom_text(aes(label = Pool), hjust = -0.2, vjust = 0.5, size = 3) +
    geom_smooth(method = "lm", se = TRUE, alpha = 0.2) +
    scale_color_manual(values = c("Illinois Waterway" = "orange", "Mississippi River" = "dodgerblue")) +
    labs(
      title = "Sample Size vs Model Performance",
      subtitle = "Does more data improve predictions?",
      x = "Number of Training Samples",
      y = "Test RMSE (ft/yr)",
      color = "River"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
  
  print(p_samples_vs_rmse)
  ggsave("./Output/Pool_Models/Samples_vs_RMSE.png", width = 12, height = 8)
  
  # Plot 4: RMSE vs RÂ² Scatter
  p_rmse_r2 <- pool_xgb_metrics |>
    mutate(river_label = ifelse(River == "IL", "Illinois Waterway", "Mississippi River")) |>
    ggplot(aes(x = Test_RMSE, y = Test_R2, color = river_label, size = N_Samples)) +
    geom_point(alpha = 0.7) +
    geom_text(aes(label = Pool), hjust = -0.2, vjust = 0.5, size = 3) +
    scale_color_manual(values = c("Illinois Waterway" = "orange", "Mississippi River" = "dodgerblue")) +
    scale_size_continuous(range = c(3, 10)) +
    labs(
      title = "Pool Model Performance: RMSE vs RÂ²",
      x = "Test RMSE (ft/yr)",
      y = "RÂ²",
      color = "River",
      size = "N Samples"
    ) +
    theme_minimal() +
    theme(legend.position = "right")
  
  print(p_rmse_r2)
  ggsave("./Output/Pool_Models/RMSE_vs_R2.png", width = 12, height = 8)
}

# ----------------------------------------------------------------------------
# 5. COMPREHENSIVE RESULTS TABLE
# ----------------------------------------------------------------------------

cat("\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n")
cat("â                 COMPREHENSIVE POOL RESULTS TABLE                       â\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n\n")

if (!is.null(pool_xgb_metrics)) {
  
  comprehensive_pool_results <- pool_xgb_metrics |>
    mutate(
      river_label = ifelse(River == "IL", "Illinois Waterway", "Mississippi River")
    ) |>
    select(
      Pool, 
      River = river_label,
      N_Samples,
      N_Gages,
      RMSE = Test_RMSE,
      MAE = Test_MAE,
      R2 = Test_R2,
      Accuracy,
      Precision,
      Recall,
      F1 = F1_Score
    ) |>
    mutate(
      RMSE = round(RMSE, 3),
      MAE = round(MAE, 3),
      R2 = round(R2, 3),
      Accuracy = round(Accuracy * 100, 1),
      Precision = round(Precision, 3),
      Recall = round(Recall, 3),
      F1 = round(F1, 3)
    ) |>
    arrange(River, RMSE)
  
  print(comprehensive_pool_results)
  
  # Save to CSV
  write_csv(comprehensive_pool_results, "./Output/Pool_Models/Comprehensive_Pool_Results.csv")
  cat("\nResults saved to ./Output/Pool_Models/Comprehensive_Pool_Results.csv\n")
}

# ----------------------------------------------------------------------------
# 6. STATISTICAL TESTS
# ----------------------------------------------------------------------------

cat("\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n")
cat("â                    STATISTICAL COMPARISONS                             â\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n\n")

if (!is.null(pool_xgb_metrics) && nrow(pool_xgb_metrics) > 1) {
  
  # Test if IWW and Miss pools have different RMSE
  iww_rmse <- pool_xgb_metrics$Test_RMSE[pool_xgb_metrics$River == "IL"]
  miss_rmse <- pool_xgb_metrics$Test_RMSE[pool_xgb_metrics$River == "UM"]
  
  if (length(iww_rmse) >= 2 && length(miss_rmse) >= 2) {
    t_test <- t.test(iww_rmse, miss_rmse)
    
    cat("=== T-Test: IWW vs Mississippi Pool RMSE ===\n")
    cat(sprintf("  IWW Mean RMSE: %.3f (n=%d)\n", mean(iww_rmse), length(iww_rmse)))
    cat(sprintf("  Miss Mean RMSE: %.3f (n=%d)\n", mean(miss_rmse), length(miss_rmse)))
    cat(sprintf("  T-statistic: %.3f\n", t_test$statistic))
    cat(sprintf("  P-value: %.4f\n", t_test$p.value))
    cat(sprintf("  Significant difference (p<0.05): %s\n", 
                ifelse(t_test$p.value < 0.05, "YES", "NO")))
  }
  
  # Correlation: Sample size vs RMSE
  cor_test <- cor.test(pool_xgb_metrics$N_Samples, pool_xgb_metrics$Test_RMSE)
  
  cat("\n=== Correlation: Sample Size vs RMSE ===\n")
  cat(sprintf("  Correlation: %.3f\n", cor_test$estimate))
  cat(sprintf("  P-value: %.4f\n", cor_test$p.value))
  cat(sprintf("  Interpretation: %s\n", 
              ifelse(cor_test$estimate < 0, 
                     "More samples â Lower RMSE (better)", 
                     "More samples â Higher RMSE (unexpected)")))
}

# ----------------------------------------------------------------------------
# 7. KEY FINDINGS SUMMARY
# ----------------------------------------------------------------------------

cat("\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n")
cat("â                       KEY FINDINGS SUMMARY                             â\n")
cat("ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n\n")

if (!is.null(pool_xgb_metrics)) {
  
  # Best overall pool
  best_pool <- pool_xgb_metrics |> slice_min(Test_RMSE, n = 1)
  worst_pool <- pool_xgb_metrics |> slice_max(Test_RMSE, n = 1)
  best_class_pool <- pool_xgb_metrics |> slice_max(F1_Score, n = 1)
  
  cat(sprintf("1. BEST REGRESSION POOL: %s (%s)\n", best_pool$Pool, 
              ifelse(best_pool$River == "IL", "Illinois Waterway", "Mississippi River")))
  cat(sprintf("   - RMSE: %.3f ft/yr, RÂ²: %.3f\n", best_pool$Test_RMSE, best_pool$Test_R2))
  
  cat(sprintf("\n2. WORST REGRESSION POOL: %s (%s)\n", worst_pool$Pool,
              ifelse(worst_pool$River == "IL", "Illinois Waterway", "Mississippi River")))
  cat(sprintf("   - RMSE: %.3f ft/yr, RÂ²: %.3f\n", worst_pool$Test_RMSE, worst_pool$Test_R2))
  
  cat(sprintf("\n3. BEST CLASSIFICATION POOL: %s (%s)\n", best_class_pool$Pool,
              ifelse(best_class_pool$River == "IL", "Illinois Waterway", "Mississippi River")))
  cat(sprintf("   - Accuracy: %.1f%%, F1: %.3f\n", best_class_pool$Accuracy * 100, best_class_pool$F1_Score))
  
  # River comparison
  iww_avg_rmse <- mean(pool_xgb_metrics$Test_RMSE[pool_xgb_metrics$River == "IL"], na.rm = TRUE)
  miss_avg_rmse <- mean(pool_xgb_metrics$Test_RMSE[pool_xgb_metrics$River == "UM"], na.rm = TRUE)
  
  cat(sprintf("\n4. RIVER COMPARISON:\n"))
  cat(sprintf("   - Illinois Waterway avg pool RMSE: %.3f ft/yr\n", iww_avg_rmse))
  cat(sprintf("   - Mississippi River avg pool RMSE: %.3f ft/yr\n", miss_avg_rmse))
  cat(sprintf("   - %s pools are more predictable on average\n",
              ifelse(iww_avg_rmse < miss_avg_rmse, "Illinois Waterway", "Mississippi River")))
  
  # Pool vs River level
  cat(sprintf("\n5. POOL vs RIVER-LEVEL:\n"))
  cat(sprintf("   - %d of %d pools outperform river-level model (IWW)\n",
              sum(pool_xgb_metrics$Test_RMSE[pool_xgb_metrics$River == "IL"] < IWW_xgb$metrics$Test_RMSE),
              sum(pool_xgb_metrics$River == "IL")))
  cat(sprintf("   - %d of %d pools outperform river-level model (Miss)\n",
              sum(pool_xgb_metrics$Test_RMSE[pool_xgb_metrics$River == "UM"] < Miss_xgb$metrics$Test_RMSE),
              sum(pool_xgb_metrics$River == "UM")))
}
```



# Visualizations

## Prediction Plots

```{r prediction-plots, fig.height=10}
# Function to create time series comparison
# Function to create time series comparison
plot_predictions <- function(pred_df, dataset_name, model_type) {
  
  plot_data <- pred_df |>
    select(date, actual, predicted) |>
    pivot_longer(cols = c(actual, predicted), names_to = "type", values_to = "shoaling_rate") |>
    mutate(type = factor(type, levels = c("actual", "predicted"), labels = c("Actual", "Predicted")))
  
  ggplot(plot_data, aes(x = date, y = shoaling_rate, color = type)) +
    geom_line(linewidth = 0.8, alpha = 0.8) +
    geom_point(size = 1.5, alpha = 0.6) +
    scale_color_manual(values = c("Actual" = "#2166AC", "Predicted" = "#D6604D")) +
    geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
    labs(
      title = sprintf("%s - %s Model", dataset_name, model_type),
      x = "Date", y = "Annual Shoaling Rate (ft/yr)", color = ""
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
}

# Function for scatter plot
plot_scatter <- function(pred_df, dataset_name, model_type) {
  
  rmse <- sqrt(mean((pred_df$predicted - pred_df$actual)^2))
  mae <- mean(abs(pred_df$predicted - pred_df$actual))
  r2 <- cor(pred_df$actual, pred_df$predicted)^2
  
  ggplot(pred_df, aes(x = actual, y = predicted)) +
    geom_point(alpha = 0.6, size = 2.5, color = "#4575B4") +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red", linewidth = 1) +
    geom_smooth(method = "lm", se = TRUE, color = "#D73027", fill = "#D73027", alpha = 0.2) +
    labs(
      title = sprintf("%s - %s", dataset_name, model_type),
      subtitle = sprintf("RMSE: %.3f | MAE: %.3f | RÂ²: %.3f", rmse, mae, r2),
      x = "Actual (ft/yr)", y = "Predicted (ft/yr)"
    ) +
    theme_minimal()
}

# xGBoost plots
p_iww_ts <- plot_predictions(IWW_xgb$predictions, "Illinois Waterway", "xGBoost")
p_miss_ts <- plot_predictions(Miss_xgb$predictions, "Mississippi River", "xGBoost")

p_iww_sc <- plot_scatter(IWW_xgb$predictions, "Illinois Waterway", "xGBoost")
p_miss_sc <- plot_scatter(Miss_xgb$predictions, "Mississippi River", "xGBoost")

(p_iww_ts + p_miss_ts) / (p_iww_sc + p_miss_sc)

ggsave("./Output/Comparisons/xGBoost_Predictions.png", width = 14, height = 12)

# LSTM plots
if (!is.null(IWW_lstm_result) && !is.null(Miss_lstm_result) && !is.null(Combined_lstm_result)) {
  
  IWW_lstm_pred_df <- data.frame(
    actual = IWW_lstm_result$test_actual[, 1],
    predicted = IWW_lstm_result$test_pred[, 1],
    date = as.Date(IWW_lstm_result$test_dates)
  )
  
  Miss_lstm_pred_df <- data.frame(
    actual = Miss_lstm_result$test_actual[, 1],
    predicted = Miss_lstm_result$test_pred[, 1],
    date = as.Date(Miss_lstm_result$test_dates)
  )
  
  Combined_lstm_pred_df <- data.frame(
    actual = Combined_lstm_result$test_actual[, 1],
    predicted = Combined_lstm_result$test_pred[, 1],
    date = as.Date(Combined_lstm_result$test_dates)
  )
  
  # IWW and Miss side by side
  p_iww_lstm_ts <- plot_predictions(IWW_lstm_pred_df, "Illinois Waterway", "LSTM")
  p_miss_lstm_ts <- plot_predictions(Miss_lstm_pred_df, "Mississippi River", "LSTM")
  
  p_iww_lstm_sc <- plot_scatter(IWW_lstm_pred_df, "Illinois Waterway", "LSTM")
  p_miss_lstm_sc <- plot_scatter(Miss_lstm_pred_df, "Mississippi River", "LSTM")
  
  print((p_iww_lstm_ts + p_miss_lstm_ts) / (p_iww_lstm_sc + p_miss_lstm_sc))
  ggsave("./Output/Comparisons/LSTM_Predictions.png", width = 14, height = 12)
  
  # Combined LSTM separately
  p_comb_lstm_ts <- plot_predictions(Combined_lstm_pred_df, "Combined", "LSTM")
  p_comb_lstm_sc <- plot_scatter(Combined_lstm_pred_df, "Combined", "LSTM")
  
  print(p_comb_lstm_ts / p_comb_lstm_sc)
  ggsave("./Output/Comparisons/Combined_LSTM_Predictions.png", width = 10, height = 12)
}
```

## LSTM Horizon Analysis
```{r lstm-horizon-plot, fig.height=6}
# Check if results exist and have horizon_metrics
if (!is.null(IWW_lstm_result$horizon_metrics) && 
    !is.null(Miss_lstm_result$horizon_metrics) &&
    !is.null(Combined_lstm_result$horizon_metrics)) {
  
  horizon_data <- bind_rows(
    IWW_lstm_result$horizon_metrics |> mutate(Dataset = "Illinois Waterway"),
    Miss_lstm_result$horizon_metrics |> mutate(Dataset = "Mississippi River"),
    Combined_lstm_result$horizon_metrics |> mutate(Dataset = "Combined")
  )
  
  ggplot(horizon_data, aes(x = Horizon, y = RMSE, color = Dataset)) +
    geom_line(linewidth = 1.2) +
    geom_point(size = 2) +
    scale_color_viridis_d() +
    labs(
      title = "LSTM Performance by Forecast Horizon",
      subtitle = "RMSE increases with forecast distance",
      x = "Forecast Horizon (observations ahead)",
      y = "RMSE (ft/yr)"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
  
  ggsave("./Output/LSTM/Horizon_Performance.png", width = 10, height = 6)
  
} else {
  cat("Horizon metrics not available for plotting.\n")
}
```

##  Pool Performance Map

```{r pool-map, fig.height=10}
if (!is.null(pool_xgb_metrics)) {
  
  # Add x_pos if not already present
  pool_info <- pool_info |>
    mutate(x_pos = ifelse(river == "IL", 2, 1))
  
  pool_map_data <- pool_xgb_metrics |>
    left_join(pool_info |> select(pool, river, center_rm, x_pos),
              by = c("Pool" = "pool", "River" = "river"))
  
  ggplot(pool_map_data, aes(x = x_pos, y = center_rm)) +
    geom_line(aes(group = River), color = "lightblue", linewidth = 8, alpha = 0.4) +
    geom_point(aes(size = N_Samples, color = Test_RMSE), alpha = 0.8) +
    geom_text(aes(label = Pool), hjust = -0.3, size = 3) +
    scale_color_viridis_c(option = "plasma", direction = -1, name = "RMSE\n(ft/yr)") +
    scale_size_continuous(range = c(3, 12), name = "N Samples") +
    scale_x_continuous(breaks = c(1, 2),
                       labels = c("Mississippi\nRiver", "Illinois\nWaterway"),
                       limits = c(0.5, 2.8)) +
    labs(
      title = "Pool-Level xGBoost Model Performance",
      subtitle = "By river mile (upstream = higher values)",
      y = "River Mile", x = ""
    ) +
    theme_minimal() +
    theme(panel.grid.major.x = element_blank())
  
  ggsave("./Output/Maps/Pool_Performance_Schematic.png", width = 10, height = 12)
}
```

# Dredging Priority Map
```{r}
create_dredging_prediction_map <- function(pool_xgb_results, 
                                           pool_info,
                                           gage_CSAT_joined,
                                           threshold_df) {
  
  # Collect predictions in a list first
  prediction_list <- list()
  
  for (key in names(pool_xgb_results)) {
    
    result <- pool_xgb_results[[key]]
    
    # Skip if no model or not successful
    if (is.null(result$model) || result$status != "success") {
      cat(sprintf("  Skipping %s: status = %s\n", key, result$status))
      next
    }
    
    pool_name <- result$pool
    river_code <- result$river
    gages_used <- result$gages_used
    
    # Get most recent data for this pool
    recent_data <- gage_CSAT_joined |>
      filter(pool == pool_name, river == river_code) |>
      arrange(desc(SurveyDateBefore)) |>
      dplyr::slice(1)
    
    if (nrow(recent_data) == 0) {
      cat(sprintf("  Skipping %s: no data found\n", key))
      next
    }
    
    # Select only the features the model needs
    model_features <- c("WeekOfYear", "DaysBetween", gages_used)
    available_features <- intersect(model_features, names(recent_data))
    
    # Create prediction dataset with only required features
    pred_data <- recent_data |>
      select(all_of(available_features))
    
    # Check for NAs
    if (any(is.na(pred_data))) {
      cat(sprintf("  Skipping %s: NA values in features\n", key))
      next
    }
    
    # Make prediction
    pred <- tryCatch({
      p <- predict(result$model, newdata = pred_data)
      if (length(p) == 0 || is.null(p)) NA else as.numeric(p)
    }, error = function(e) {
      cat(sprintf("  Skipping %s: prediction error - %s\n", key, e$message))
      NA
    })
    
    if (is.na(pred)) next
    
    # Get threshold
    thresh <- threshold_df |>
      filter(river == river_code) |>
      pull(mean_rate)
    
    if (length(thresh) == 0) thresh <- comb_thresh
    
    # Add to list
    prediction_list[[key]] <- data.frame(
      Pool = pool_name,
      River = river_code,
      Last_Survey = recent_data$SurveyDateBefore,
      Actual_Rate = recent_data$AnnualShoalingRate_ftperyr,
      Predicted_Rate = pred,
      Threshold = thresh,
      Dredge_Needed = pred > thresh,
      Urgency = case_when(
        pred > thresh * 2 ~ "HIGH",
        pred > thresh ~ "MODERATE", 
        pred > 0 ~ "LOW",
        TRUE ~ "NONE"
      )
    )
    
    cat(sprintf("  Success: %s - Predicted: %.2f\n", key, pred))
  }
  
  # Check if we got any predictions
  if (length(prediction_list) == 0) {
    warning("No predictions generated")
    return(NULL)
  }
  
  # Combine into data frame
  prediction_data <- do.call(rbind, prediction_list)
  
  # Join with pool location info
  prediction_data <- prediction_data |>
    left_join(
      pool_info |> select(pool, river, center_rm, n_samples),
      by = c("Pool" = "pool", "River" = "river")
    ) |>
    mutate(
      x_pos = ifelse(River == "IL", 2, 1),
      river_label = ifelse(River == "IL", "Illinois Waterway", "Mississippi River")
    )
  
  return(prediction_data)
}

# Generate predictions
cat("Generating dredging predictions...\n")
dredge_predictions <- create_dredging_prediction_map(
  pool_xgb_results = pool_xgb_results,
  pool_info = pool_info,
  gage_CSAT_joined = gage_CSAT_joined,
  threshold_df = river_thresholds
)

# Check results
if (!is.null(dredge_predictions)) {
  cat(sprintf("\nGenerated %d predictions\n", nrow(dredge_predictions)))
  print(dredge_predictions |> select(Pool, River, Predicted_Rate, Urgency))
} else {
  cat("\nNo predictions generated - see messages above\n")
}
# ============================================================================
# PLOT: Dredging Need by Pool
# ============================================================================

if (!is.null(dredge_predictions) && nrow(dredge_predictions) > 0) {

  p_dredge_map <- ggplot(dredge_predictions, aes(x = x_pos, y = center_rm)) +
    # River lines
    geom_line(aes(group = River), color = "lightblue", linewidth = 10, alpha = 0.3) +
    # Pool points - colored by urgency
    geom_point(aes(size = abs(Predicted_Rate), fill = Urgency), 
               shape = 21, color = "black", alpha = 0.8) +
    # Pool labels
    geom_text(aes(label = Pool), hjust = -0.3, size = 3) +
    # Color scale for urgency
    scale_fill_manual(
      values = c("HIGH" = "#d73027", "MODERATE" = "#fc8d59", 
                 "LOW" = "#fee090", "NONE" = "#91bfdb"),
      name = "Dredging\nUrgency"
    ) +
    scale_size_continuous(range = c(4, 15), name = "Predicted\nRate (ft/yr)") +
    scale_x_continuous(
      breaks = c(1, 2),
      labels = c("Mississippi\nRiver", "Illinois\nWaterway"),
      limits = c(0.5, 2.8)
    ) +
    labs(
      title = "Predicted Dredging Needs by Pool",
      subtitle = paste("Based on most recent survey data | Generated:", Sys.Date()),
      y = "River Mile",
      x = ""
    ) +
    theme_minimal() +
    theme(
      panel.grid.major.x = element_blank(),
      legend.position = "right"
    )

  print(p_dredge_map)
  ggsave("./Output/Maps/Dredging_Predictions_Map.png", width = 12, height = 14)

  # ============================================================================
  # TABLE: Dredging Priority List
  # ============================================================================

  dredge_priority <- dredge_predictions |>
    filter(Dredge_Needed == TRUE) |>
    arrange(desc(Predicted_Rate)) |>
    select(Pool, river_label, Predicted_Rate, Threshold, Urgency, Last_Survey) |>
    mutate(
      Predicted_Rate = round(Predicted_Rate, 2),
      Days_Since_Survey = as.numeric(Sys.Date() - Last_Survey)
    )

  cat("\n=== DREDGING PRIORITY LIST ===\n")
  print(dredge_priority)

  # Save priority list
  write_csv(dredge_priority, "./Output/Maps/Dredging_Priority_List.csv")

} else {
  cat("No dredging predictions available to map\n")
}

```

# Save Results

```{r save-results}
# Save regression results
write_csv(river_regression, "./Output/Comparisons/River_Level_Results.csv")

# Save classification results
write_csv(classification_results, "./Output/Comparisons/Classification_Results.csv")

# Save pool results
if (!is.null(pool_xgb_metrics)) {
  write_csv(pool_xgb_metrics, "./Output/Pool_Models/Pool_xGBoost_Results.csv")
}

if (!is.null(pool_lstm_metrics)) {
  write_csv(pool_lstm_metrics, "./Output/Pool_Models/Pool_LSTM_Results.csv")
}

# Save pool-gage mapping
mapping_df <- data.frame(
  Key = names(pool_gage_mapping),
  Pool = sapply(pool_gage_mapping, function(x) x$pool),
  River = sapply(pool_gage_mapping, function(x) x$river_code),
  N_Samples = sapply(pool_gage_mapping, function(x) x$n_samples),
  N_Gages = sapply(pool_gage_mapping, function(x) x$n_selected_gages),
  Gages = sapply(pool_gage_mapping, function(x) paste(x$selected_gages, collapse = ", "))
)
write_csv(mapping_df, "./Output/Pool_Models/Pool_Gage_Mapping.csv")

cat("Results saved to ./Output/\n")
```

# Recommendations

```{r recommendations}
generate_recommendations <- function(river_name, regression_data, classification_data) {
  
  best_ml <- regression_data |>
    filter(River == river_name, Type == "ML") |>
    slice_min(Test_RMSE, n = 1)
  
  best_baseline <- regression_data |>
    filter(River == river_name, Type == "Baseline") |>
    slice_min(Test_RMSE, n = 1)
  
  best_class <- classification_data |>
    filter(Dataset == river_name) |>
    slice_max(F1_Score, n = 1)
  
  if (nrow(best_ml) == 0 || nrow(best_baseline) == 0) {
    cat(sprintf("\n=== %s ===\n", river_name))
    cat("  Insufficient data for recommendations\n")
    return()
  }
  
  improvement <- (best_baseline$Test_RMSE - best_ml$Test_RMSE) / best_baseline$Test_RMSE * 100
  
  cat(sprintf("\n=== %s ===\n", river_name))
  cat(sprintf("Best Regression Model: %s (RMSE: %.4f, %.1f%% improvement)\n",
              best_ml$Model, best_ml$Test_RMSE, improvement))
  
  if (nrow(best_class) > 0) {
    cat(sprintf("Best Classification Model: %s (F1: %.3f)\n",
                best_class$Model, best_class$F1_Score))
  }
  
  if (best_ml$Model == "LSTM") {
    cat("  â Use for: Long-range planning (30-45 days ahead)\n")
  } else {
    cat("  â Use for: Real-time predictions, operational deployment\n")
  }
}

generate_recommendations("Illinois Waterway", river_regression, classification_results)
generate_recommendations("Mississippi River", river_regression, classification_results)
generate_recommendations("Combined", river_regression, classification_results)
```

# Stop Parallel Processing
```{r cleanup}
# Stop parallel processing cluster
stopCluster(cl)
```
